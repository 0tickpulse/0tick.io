---
sidebar_position: 3
---

import { Mafs, Coordinates, Plot, Line, Circle, Theme, useMovablePoint, useStopwatch, vec, Vector, LaTeX, Polygon, Transform } from "mafs";

import { useState, useCallback } from "react";
import { lineLabel } from "@site/src/utilities/lines";
import { color } from "@site/src/utilities/colors"
import TOCInline from '@theme/TOCInline';
import * as MB from "mathbox-react"
import * as THREE from "three"
import { OrbitControls } from "three/examples/jsm/controls/OrbitControls"

# Dot Product: Equivalence of Definitions

We have operated with the dot product as an operation with two different definitions.
In this section, we will show, in multiple different ways, that these definitions are equivalent.

## Table of Contents

<TOCInline toc={toc} />

## The Definitions to Equate

The dot product of two vectors $\va{v}$ and $\va{w}$ can be defined in two ways, as we have previously seen:

$$
\begin{align}
\va{v} \cdot \va{w} &= \norm{\va{v}} \norm{\va{w}} \cos \theta \\
\va{v} \cdot \va{w} &= v_1 w_1 + v_2 w_2 + \cdots + v_n w_n
\end{align}
$$

We will show that these two definitions are equivalent. That is:

<Boxed>
$$
\begin{equation}
\norm{\va{v}} \norm{\va{w}} \cos \theta = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n \label{eq:dot-product-equivalence}
\end{equation}
$$
</Boxed>

## By the Cosine Rule

:::info Brief Overview

This proof assumes the component definition of the dot product ($\va{v} \cdot \va{w} = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n$) and uses the cosine rule to derive the geometric definition of the dot product ($\va{v} \cdot \va{w} = \norm{\va{v}} \norm{\va{w}} \cos \theta$).

Additionally, it uses the corollary that the square of the magnitude of a vector is the dot product of the vector with itself; $\norm{\va{v}}^2 = \va{v} \cdot \va{v}$.

:::

For any triangle with sides $a$, $b$, and $c$, and angles $A$, $B$, and $C$, the cosine rule states:

$$
\begin{equation}
c^2 = a^2 + b^2 - 2ab \cos C \label{eq:cosine-rule}
\end{equation}
$$

We can apply this rule to the triangle formed by the vectors $\va{v}$ and $\va{w}$, and their difference $\va{v} - \va{w}$.
Below is a diagram of the triangle:

<Mafs
    viewBox={{
        x: [0, 3],
        y: [0, 3]
    }}
>
    <Vector
        tail={[0, 0]}
        tip={[2, 3]}
        color={color("red")}
    />
    <LaTeX
        tex="\mathbf{\vec{v}}"
        at={lineLabel([0, 0], [2, 3], "left")}
        color={color("red")}
    />

    <Vector
        tail={[0, 0]}
        tip={[3, 1]}
        color={color("blue")}
    />
    <LaTeX
        tex="\mathbf{\vec{w}}"
        at={lineLabel([0, 0], [3, 1], "right")}
        color={color("blue")}
    />

    <Angle
        at={[0, 0]}
        fromRad={Math.atan2(3, 2)}
        toRad={Math.atan2(1, 3)}
        label="\theta"
    />

    <Line.Segment
        point1={[3, 1]}
        point2={[2, 3]}
        style="dashed"
    />
    <LaTeX
        tex="\mathbf{\vec{v}} - \mathbf{\vec{w}}"
        at={lineLabel([3, 1], [2, 3], "right")}
    />
</Mafs>

If we let $a = \norm{\va{v}}$, $b = \norm{\va{w}}$, and $c = \norm{\va{v} - \va{w}}$, we can apply the cosine rule to the triangle:

$$
\begin{align}
c^2 &= \class{red}{a}^2 + \class{blue}{b}^2 - 2\class{red}{a}\class{blue}{b} \cos C \tag{\ref{eq:cosine-rule}} \\
\norm{\va{v} - \va{w}}^2 &= \class{red}{\norm{\va{v}}}^2 + \class{blue}{\norm{\va{w}}}^2 - 2\class{red}{\norm{\va{v}}}\class{blue}{\norm{\va{w}}} \cos \theta
\end{align}
$$

Recall that the square of the magnitude of a vector is the dot product of the vector with itself:

$$
\begin{split}
\norm{\va{v} - \va{w}}^2 &= (\va{v} - \va{w}) \cdot (\va{v} - \va{w}) \\
&= \va{v} \cdot \va{v} - 2\va{v} \cdot \va{w} + \va{w} \cdot \va{w} \\
&= \norm{\va{v}}^2 - 2\va{v} \cdot \va{w} + \norm{\va{w}}^2
\end{split}
$$

Substituting this back into the cosine rule equation, we get:

$$
\begin{equation}
\norm{\va{v}}^2 - 2\va{v} \cdot \va{w} + \norm{\va{w}}^2 = \norm{\va{v}}^2 + \norm{\va{w}}^2 - 2\norm{\va{v}}\norm{\va{w}} \cos \theta
\end{equation}
$$

Finally, we can cancel out the $\norm{\va{v}}^2$ and $\norm{\va{w}}^2$ terms to get the equivalence of the dot product definitions:

$$
\begin{align}
- 2\va{v} \cdot \va{w} &= - 2\norm{\va{v}}\norm{\va{w}} \cos \theta \\
\va{v} \cdot \va{w} &= \norm{\va{v}}\norm{\va{w}} \cos \theta \\
v_1 w_1 + v_2 w_2 + \cdots + v_n w_n &= \norm{\va{v}}\norm{\va{w}} \cos \theta \tag{\ref{eq:dot-product-equivalence}}
\end{align}
$$

Hence, the equivalence is proven. $\blacksquare$

## By Vector Decomposition

:::info Brief Overview

This proof assumes the geometric definition of the dot product ($\va{v} \cdot \va{w} = \norm{\va{v}} \norm{\va{w}} \cos \theta$) and uses the decomposition of vectors into their components to derive the component definition of the dot product ($\va{v} \cdot \va{w} = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n$).
Along the way, it uses various properties of the dot product, such as its distributivity over vector addition and scalar multiplication.

:::

Let $\va{v} = v_1 \ihatc + v_2 \jhatc + v_3 \khatc$ and $\va{w} = w_1 \ihatc + w_2 \jhatc + w_3 \khatc$.
As we know, these vectors are in 3D space, but the proof can be generalized to any dimension.

We can express the dot product of $\va{v}$ and $\va{w}$ as:

$$
\begin{equation}
\va{v} \cdot \va{w} = (v_1 \ihatc + v_2 \jhatc + v_3 \khatc) \cdot (w_1 \ihatc + w_2 \jhatc + w_3 \khatc)
\end{equation}
$$

By the distributivity of the dot product over vector addition, we can expand this expression into the sum of the dot products of the individual components.
This is analogous to how we expand the product of two binomials; $(a + b)(c + d) = ac + ad + bc + bd$.

Notice that in the product of the two binomials, each term is the product of one term from the first binomial and one term from the second binomial.
We get every possible combination of terms from the two binomials. This is the same for trinomials; $(a + b + c)(d + e + f) = ad + ae + af + bd + be + bf + cd + ce + cf$.

Applying this to the dot product of $\va{v}$ and $\va{w}$, we get a similar expansion:

$$
\begin{equation}
\begin{split}
\va{v} \cdot \va{w} &= (v_1 \ihatc + v_2 \jhatc + v_3 \khatc) \cdot (w_1 \ihatc + w_2 \jhatc + w_3 \khatc) \\
&= v_1 \ihatc \cdot (w_1 \ihatc + w_2 \jhatc + w_3 \khatc) + v_2 \jhatc \cdot (w_1 \ihatc + w_2 \jhatc + w_3 \khatc) + v_3 \khatc \cdot (w_1 \ihatc + w_2 \jhatc + w_3 \khatc) \\
&= \quad v_1 w_1 \ihatc \cdot \ihatc + v_1 w_2 \ihatc \cdot \jhatc + v_1 w_3 \ihatc \cdot \khatc \\
    &\quad {}+ v_2 w_1 \jhatc \cdot \ihatc + v_2 w_2 \jhatc \cdot \jhatc + v_2 w_3 \jhatc \cdot \khatc \\
    &\quad {}+ v_3 w_1 \khatc \cdot \ihatc + v_3 w_2 \khatc \cdot \jhatc + v_3 w_3 \khatc \cdot \khatc
\end{split}
\end{equation}
$$

Recall from the [previous section on the corollaries of the dot product](./Dot%20Product%20Corollaries#basis-vectors-and-the-kronecker-delta) that the dot product of two basis vectors is 1 if they are the same and 0 if they are different.
As such, all the terms in the expansion where the basis vectors are different will evaluate to 0, and the terms where the basis vectors are the same will evaluate to 1:

$$
\begin{equation}
\begin{split}
\va{v} \cdot \va{w} &= \quad \boxed{v_1 w_1 \ihatc \cdot \ihatc} + \cancel{v_1 w_2 \ihatc \cdot \jhatc} + \cancel{v_1 w_3 \ihatc \cdot \khatc} \\
    &\quad {}+ \cancel{v_2 w_1 \jhatc \cdot \ihatc} + \boxed{v_2 w_2 \jhatc \cdot \jhatc} + \cancel{v_2 w_3 \jhatc \cdot \khatc} \\
    &\quad {}+ \cancel{v_3 w_1 \khatc \cdot \ihatc} + \cancel{v_3 w_2 \khatc \cdot \jhatc} + \boxed{v_3 w_3 \khatc \cdot \khatc} \\
    &= v_1 w_1 + v_2 w_2 + v_3 w_3
\end{split}
\end{equation}
$$

Since this proof starts with the geometric definition of the dot product and derives the component definition, it shows that the two definitions are equivalent:

$$
\begin{equation}
\norm{\va{v}} \norm{\va{w}} \cos \theta = v_1 w_1 + v_2 w_2 + v_3 w_3 \tag{\ref{eq:dot-product-equivalence}}
\end{equation}
$$

Hence, the equivalence is proven. $\blacksquare$

As mentioned earlier, this proof can be generalized to any dimension using the same principles.
The key idea is to expand the dot product of two vectors into the sum of the dot products of their components, and then use the properties of the dot product to simplify the expression.

There are a few interesting things to note about this proof:

1. Notice that this is independent of the cosine rule.
    This means that we can actually *derive the cosine rule from the dot product*, rather than the other way around.
2. This proof gives you an explicit formula to get the magnitude of a vector.
    In the previous proof, we just assumed that the magnitude squared gives you $v_1^2 + v_2^2 + v_3^2$.
    But this proof shows this independently.

    Not only is this also a proof of the Pythagorean theorem, but this proof is also especially important when we consider vector spaces without an orthonormal basis.
    In such cases, the magnitude is not just $\sqrt{v_1^2 + v_2^2 + v_3^2}$, but something else.
    These situations are more common than one would think, especially in fields such as relativity and quantum mechanics.
    With that in mind, we say that the magnitude, or norm, of a vector is actually *defined* via the dot product.

## Dot Product as a Linear Transformation

:::info Brief Overview

This proof assumes we don't know the geometric or component definitions of the dot product.
Instead, it treats the act of projection as a linear transformation and shows that the computation of this transformation is equivalent to the component definition of the dot product.

:::

This proof originates from 3b1b's series on linear algebra. It's a very interesting way to think about the dot product.

Imagine that we do not know anything about the dot product, only that it does some kind of projection.
Let $\vu{u}$ be some unit vector that we want to project $\va{v}$ onto. Essentially, we are turning this two-dimensional $\va{v}$ onto a line defined by $\vu{u}$.
Hence, this is a transformation from $\mathbb{R}^2$ to $\mathbb{R}$.

<LinearTransformationVisualizer
    to={{
        ihat: vec.withMag([3, 2], vec.dot(vec.normalize([3, 2]), vec.normalize([1, 0]))),
        jhat: vec.withMag([3, 2], vec.dot(vec.normalize([3, 2]), vec.normalize([0, 1])))
    }}
>
    <Plot.OfX y={x => (2 / 3) * x} />
</LinearTransformationVisualizer>

(Note that this line is just a visual representation of the projection; it does not actually "sit" in this 2D space.)

Recall that linear transformations are defined by how they act on the basis vectors.
Since we want to project it onto a line, each column in the matrix representing the transformation would only have one entry.
Call this matrix $\vb{P}$:

$$
\begin{equation}
\vb{P} = \mqty[? & ?]
\end{equation}
$$

Recall that each column in the matrix represents the result of applying the transformation to the basis vectors.
To find these columns, we can zoom in on the diagram above. Below is the projection of $\ihatc$ onto the line:

<Mafs viewBox={{ x: [0, 1], y: [0, 0.5] }}>
    <Coordinates.Cartesian />
    <Plot.OfX y={x => (2 / 3) * x} />

    {/* ihat */}
    <Vector tip={[1, 0]} color={color("blue")} />
    <LaTeX tex="\mathbf{\hat{i}}" at={[0.5, -0.1]} color={color("blue")} />

    {/* uhat */}
    <Vector tip={vec.normalize([3, 2])} color={color("green")} weight={3} />
    <LaTeX tex="\mathbf{\hat{u}}" at={[0.7, 0.6]} color={color("green")} />

    {/* projection */}
    <Vector tip={vec.withMag([3, 2], vec.dot([1, 0], [3, 2]) / vec.mag([3, 2]))} color={color("red")} />
    <LaTeX tex="T(\mathbf{\hat{i}})" at={[0.5, 0.45]} color={color("red")} />

    {/* dotted line connecting the two */}
    <Line.Segment point1={[1, 0]} point2={vec.withMag([3, 2], vec.dot([1, 0], [3, 2]) / vec.mag([3, 2]))} style="dashed" />
</Mafs>

Here's the key part: the projection of $\ihatc$ onto the line, $\vu{u}$, is *exactly* the same as the projection of $\vu{u}$ onto $\ihatc$.
We can see this visually by creating a line of symmetry between the two vectors:

<Mafs viewBox={{ x: [0, 1], y: [0, 0.5] }}>
    <Coordinates.Cartesian />
    <Plot.OfX y={x => (2 / 3) * x} />

    {/* line of symmetry, halfway angle between x-axis and y = 2/3 x */}
    <Plot.OfX y={x => Math.tan(Math.atan(2 / 3) / 2) * x} style="dashed" />

    {/* ihat */}
    <Vector tip={[1, 0]} color={color("blue")} />
    <LaTeX tex="\mathbf{\hat{i}}" at={[0.5, -0.1]} color={color("blue")} />

    {/* uhat */}
    <Vector tip={vec.normalize([3, 2])} color={color("green")} weight={3} />
    <LaTeX tex="\mathbf{\hat{u}}" at={[0.7, 0.6]} color={color("green")} />

    {/* projection of i onto u */}
    <Vector tip={vec.withMag([3, 2], vec.dot([1, 0], [3, 2]) / vec.mag([3, 2]))} color={color("red")} />
    <LaTeX tex="\mathbf{\hat{i}} \text{ on } \mathbf{\hat{u}}" at={[0.5, 0.45]} color={color("red")} />

    {/* projection of u onto i */}
    <Vector tip={vec.withMag([1, 0], vec.dot(vec.normalize([3, 2]), vec.normalize([1, 0])))} color={color("purple")} />
    <LaTeX tex="\mathbf{\hat{u}} \text{ on } \mathbf{\hat{i}}" at={[0.6, 0.1]} color={color("purple")} />

    <Line.Segment point1={[1, 0]} point2={vec.withMag([3, 2], vec.dot([1, 0], [3, 2]) / vec.mag([3, 2]))} style="dashed" />
    <Line.Segment point1={vec.normalize([3, 2])} point2={vec.withMag([1, 0], vec.dot(vec.normalize([3, 2]), vec.normalize([1, 0])))} style="dashed" />
</Mafs>

But the projection of $\vu{u}$ onto $\ihatc$ is just the $x$-component of $\vu{u}$:

<Mafs viewBox={{ x: [0, 1], y: [0, 0.5] }}>
    <Coordinates.Cartesian />
    <Plot.OfX y={x => (2 / 3) * x} />

    {/* uhat */}
    <Vector tip={vec.normalize([3, 2])} color={color("green")} weight={3} />
    <LaTeX tex="\mathbf{\hat{u}}" at={[0.7, 0.6]} color={color("green")} />

    {/* projection of u onto i */}
    <Vector tip={vec.withMag([1, 0], vec.dot(vec.normalize([3, 2]), vec.normalize([1, 0])))} color={color("purple")} />
    <LaTeX tex="u_x" at={[0.6, 0.1]} color={color("purple")} />

    <Line.Segment point1={vec.normalize([3, 2])} point2={vec.withMag([1, 0], vec.dot(vec.normalize([3, 2]), vec.normalize([1, 0])))} style="dashed" />
</Mafs>

Thus, the projection of $\ihatc$ onto $\vu{u}$ is the same as the projection of $\vu{u}$ onto $\ihatc$, which is $u_x$.
The same applies for the projection of $\jhatc$ onto $\vu{u}$, which is $u_y$.
Thus, the matrix $\vb{P}$ is:

$$
\begin{equation}
\vb{P} = \mqty[u_x & u_y]
\end{equation}
$$

Notice what happened here: we have a transformation that takes a vector and projects it onto a line.
This transformation is represented by a matrix, and the matrix is the dot product of the vector with the basis vectors.
But notice that this transformation is exactly the same as what the dot product does.

Now, we can apply this transformation to any arbitrary vector $\va{v} = v_1 \ihatc + v_2 \jhatc$.

$$
\begin{equation}
\vb{P} \mqty[v_1 \\ v_2] = \mqty[u_x & u_y] \mqty[v_1 \\ v_2] = u_x v_1 + u_y v_2
\end{equation}
$$

This is the same as the component definition of the dot product.

Hence, the equivalence is proven. $\blacksquare$

Note: What we have derived here has a name: covectors, or dual vectors.
Essentially, when a linear transformation maps to $\mathbb{R}$, it can be represented as a row vector.
Then, if we "tip" this row vector over, we can express this transformation as a dot product.
This vector that corresponds to the linear transformation is precisely what we call a covector.

## Non-Orthonormal Vectors (Optional Section)

The component definition of the dot product, as shown in the second proof, is only valid for orthonormal vectors.
However, the geometric definition of the dot product, as shown in the first proof, is valid for any pair of vectors because it is independent of the basis.

We can try to extend the component definition to non-orthonormal vectors.
Let $\va{v} = v_1 \va{e}_1 + v_2 \va{e}_2 + v_3 \va{e}_3$ and $\va{w} = w_1 \va{e}_1 + w_2 \va{e}_2 + w_3 \va{e}_3$.
The dot product of $\va{v}$ and $\va{w}$ is, by distributing everything:

$$
\begin{equation}
\begin{split}
\va{v} \cdot \va{w} &= \class{red}{v_1} \class{blue}{w_1} \va{e}_1 \cdot \va{e}_1 + \class{red}{v_1} \class{blue}{w_2} \va{e}_1 \cdot \va{e}_2 + \class{red}{v_1} \class{blue}{w_3} \va{e}_1 \cdot \va{e}_3 \\
&\quad {}+ \class{red}{v_2} \class{blue}{w_1} \va{e}_2 \cdot \va{e}_1 + \class{red}{v_2} \class{blue}{w_2} \va{e}_2 \cdot \va{e}_2 + \class{red}{v_2} \class{blue}{w_3} \va{e}_2 \cdot \va{e}_3 \\
&\quad {}+ \class{red}{v_3} \class{blue}{w_1} \va{e}_3 \cdot \va{e}_1 + \class{red}{v_3} \class{blue}{w_2} \va{e}_3 \cdot \va{e}_2 + \class{red}{v_3} \class{blue}{w_3} \va{e}_3 \cdot \va{e}_3
\end{split}
\end{equation}
$$

Notice that each row has a constant index for $v$ (red) and a varying index for $w$ (blue).
We can group the terms by the the $v$ index by a product between a row vector and a column vector:

$$
\begin{equation}
\va{v} \cdot \va{w} = \mqty[\class{red}{v_1} & \class{red}{v_2} & \class{red}{v_3}]
    \mqty[
        \class{blue}{w_1} \va{e}_1 \cdot \va{e}_1 + \class{blue}{w_2} \va{e}_1 \cdot \va{e}_2 + \class{blue}{w_3} \va{e}_1 \cdot \va{e}_3 \\
        \class{blue}{w_1} \va{e}_2 \cdot \va{e}_1 + \class{blue}{w_2} \va{e}_2 \cdot \va{e}_2 + \class{blue}{w_3} \va{e}_2 \cdot \va{e}_3 \\
        \class{blue}{w_1} \va{e}_3 \cdot \va{e}_1 + \class{blue}{w_2} \va{e}_3 \cdot \va{e}_2 + \class{blue}{w_3} \va{e}_3 \cdot \va{e}_3
    ]
\end{equation}
$$

We can further group the terms by the $w$ index by a product between a matrix and a column vector:

<Boxed>
$$
\begin{equation}
\va{v} \cdot \va{w} = \mqty[\class{red}{v_1} & \class{red}{v_2} & \class{red}{v_3}]
    \mqty[
        \va{e}_1 \cdot \va{e}_1 & \va{e}_1 \cdot \va{e}_2 & \va{e}_1 \cdot \va{e}_3 \\
        \va{e}_2 \cdot \va{e}_1 & \va{e}_2 \cdot \va{e}_2 & \va{e}_2 \cdot \va{e}_3 \\
        \va{e}_3 \cdot \va{e}_1 & \va{e}_3 \cdot \va{e}_2 & \va{e}_3 \cdot \va{e}_3
    ]
    \mqty[\class{blue}{w_1} \\ \class{blue}{w_2} \\ \class{blue}{w_3}]
\end{equation}
$$
</Boxed>

This is the most general form of the dot product, and it is valid for any pair of vectors, orthonormal or not.
The matrix in the middle is called the **metric tensor matrix**. It is a matrix that encodes the information about the dot product of the basis vectors.
If we denote this matrix as $\vb{G}$, then the dot product of $\va{v}$ and $\va{w}$ can be written as:

<Boxed>
$$
\begin{equation}
\va{v} \cdot \va{w} = \va{v}^T \vb{G} \va{w}
\end{equation}
$$
</Boxed>

It is often convenient to refer to a specific part of the metric tensor matrix.
The element in the $i$th row and $j$th column of the metric tensor matrix is denoted as $g_{ij}$:

$$
\begin{equation}
g_{ij} \equiv \va{e}_i \cdot \va{e}_j
\end{equation}
$$

Then, we can write the dot product as a sum over the components of the vectors:

<Boxed>
$$
\begin{equation}
\va{v} \cdot \va{w} = \sum_{i=1}^n \sum_{j=1}^n v_i g_{ij} w_j
\end{equation}
$$
</Boxed>

In the case of orthonormal vectors, we simply have $g_{ij} = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta.
This follows from the fact that the dot product of two basis vectors is 1 if they are the same and 0 if they are different.
The matrix is then the identity matrix, and the dot product simplifies to $\va{v} \cdot \va{w} = \va{v}^T \va{w} = v_1 w_1 + v_2 w_2 + \cdots + v_n w_n$.

## Summary and Next Steps

In this section, we have shown that the two definitions of the dot product are equivalent.
This is a very important result, as it shows that the dot product is a very fundamental operation in linear algebra.

Here are the key points to remember:

- The geometric definition of the dot product, $\norm{\va{v}} \norm{\va{w}} \cos \theta$, is equivalent to the component definition, $v_1 w_1 + v_2 w_2 + \cdots + v_n w_n$.
- The equivalence can be shown in multiple ways, such as using the cosine rule, vector decomposition, or treating the dot product as a linear transformation.
- The component definition of the dot product can be extended to non-orthonormal vectors using the metric tensor matrix:

    $$
    \begin{equation}
    \va{v} \cdot \va{w} = \va{v}^T \vb{G} \va{w} = \sum_{i=1}^n \sum_{j=1}^n v_i g_{ij} w_j
    \end{equation}
    $$

In the next section, we will end our introduction to the dot product by exploring some of its real-world applications.
