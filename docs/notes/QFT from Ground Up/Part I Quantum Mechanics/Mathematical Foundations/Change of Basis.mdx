---
sidebar_position: 6
---

import { Mafs, Coordinates, Plot, Line, Circle, Theme, useMovablePoint, useStopwatch, vec, Vector, LaTeX, Polygon, Transform } from "mafs";

import { useState, useCallback } from "react";
import { lineLabel } from "@site/src/utilities/lines";
import { color } from "@site/src/utilities/colors"
import TOCInline from '@theme/TOCInline';
import * as MB from "mathbox-react"
import * as THREE from "three"
import { OrbitControls } from "three/examples/jsm/controls/OrbitControls"

# Change of Basis

There are many cases in quantum mechanics where we need to change the basis of a vector from one basis to another.
For example, in our discussion of spin states, we can use $\ket{S_z; +}$ and $\ket{S_z; -}$ as the basis vectors, but we can also use $\ket{S_x; +}$ and $\ket{S_x; -}$ as the basis vectors.

The change of basis is very important in many areas of physics.
For instance, the entirety of special relativity just boils down to changing the basis of spacetime vectors.
We now explore some mathematical concepts that will help us understand how to change the basis of a vector.

## Table of Contents

<TOCInline toc={toc} />

## The Transformation Matrix

Suppose $\ket{a_1}, \ket{a_2}, \ldots, \ket{a_n}$ and $\ket{b_1}, \ket{b_2}, \ldots, \ket{b_n}$ are two sets of orthonormal basis vectors.
In order to transform one basis to another, we need to find an operator that can transform the basis vectors.

Denote this operator as $\hat{U}$, such that $U\ket{a_i} = \ket{b_i}$.
In order to find out what this operator is, we can use a neat trick using the outer product.

Consider the following: $\dyad{b_i}{a_i} \ket{a_j}$.
When $i = j$, this is just $\ket{b_i} \braket{a_i} = \ket{b_i}$.
When $i \neq j$, this is $\ket{b_i} \braket{a_i}{a_j} = 0$.
Therefore, we can write $\hat{U}$ as a sum of all of these outer products:

$$
\begin{equation}
U = \sum_{i = 1}^{n} \dyad{b_i}{a_i}
\end{equation}
$$

### Unitarity

Previously, we have seen that a Hermitian operator $A$ satisfies $A^\dagger = A$.
Let's see what happens when we take the adjoint of $\hat{U}$, remembering the rules of adjoints:

$$
\begin{equation}
\begin{split}
\hat{U}^\dagger &= \qty(\sum_{i = 1}^{n} \dyad{b_i}{a_i})^\dagger \\
&= \sum_{i = 1}^{n} \qty(\dyad{b_i}{a_i})^\dagger \\
&= \sum_{i = 1}^{n} \dyad{a_i}{b_i}
\end{split}
\end{equation}
$$

Notice what happens when we multiply $\hat{U}$ and $U^\dagger$:

$$
\begin{equation}
\begin{split}
\hat{U}^\dagger \hat{U} &= \qty(\sum_{i = 1}^{n} \dyad{a_i}{b_i}) \qty(\sum_{j = 1}^{n} \dyad{b_j}{a_j}) \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} \dyad{a_i}{b_i} \dyad{b_j}{a_j} \\
&= \sum_{i = 1}^{n} \sum_{j = 1}^{n} \dyad{a_i}{a_i} \delta_{ij}
\end{split}
\end{equation}
$$

Since $\delta_{ij}$ ensures that the sum is only nonzero when $i = j$, we can remove one of the sums:

$$
\begin{equation}
\begin{split}
\hat{U}^\dagger \hat{U} &= \sum_{i = 1}^{n} \dyad{a_i}{a_i} \\
&= I
\end{split}
\end{equation}
$$

where we have used the completeness relation $\sum_{i = 1}^{n} \dyad{a_i}{a_i} = I$.
Therefore, $\hat{U}^\dagger \hat{U} = I$. We call such an operator $\hat{U}$ an **unitary operator**.
Unitary operators will be very important when we discuss the time-evolution of quantum states.

We can summarize what we have found in the following theorem:

<Boxed>
Given two sets of orthonormal and complete basis vectors $\ket{a_i}$ and $\ket{b_i}$, there always exists a unitary operator $\hat{U}$ that transforms one basis to another such that $\hat{U}\ket{a_i} = \ket{b_i}$.
</Boxed>

### Matrix Representation

As always, we can represent operators as matrices, and the change of basis operator $\hat{U}$ is no exception.
Recall that the $ij$-th element of the matrix representation of an operator is $\mel{E_i}{\hat{A}}{E_j}$.
Therefore, the matrix representation of $\hat{U}$ is:

$$
\begin{equation}
U_{ij} = \mel{a_i}{\hat{U}}{a_j} = \braket{b_i}{a_j}
\end{equation}
$$

It is insightful to see that this is the same as the transformation matrix we have seen in linear algebra:

$$
\begin{equation}
T = \mqty[\vu{x} \cdot \tilde{\vu{x}} & \vu{x} \cdot \tilde{\vu{y}} & \vu{x} \cdot \tilde{\vu{z}} \\ \vu{y} \cdot \tilde{\vu{x}} & \vu{y} \cdot \tilde{\vu{y}} & \vu{y} \cdot \tilde{\vu{z}} \\ \vu{z} \cdot \tilde{\vu{x}} & \vu{z} \cdot \tilde{\vu{y}} & \vu{z} \cdot \tilde{\vu{z}}]
\end{equation}
$$

where $\vu{x}, \vu{y}, \vu{z}$ and $\tilde{\vu{x}}, \tilde{\vu{y}}, \tilde{\vu{z}}$ are two sets of orthonormal basis vectors.
In the case of curvilinear coordinates, we can use a combination of the metric tensor and the Jacobian to find the transformation matrix.

### Components of Vectors Under Change of Basis

The next question we might ask is: how do the components of a vector change under a change of basis?
If one is familiar with tensors, one might recall that vectors have contravariant components; the components transform in the opposite way to the basis vectors.

It is easy to see why this is the case.
I borrow from Eigenchris on YouTube for this explanation.
Consider a vector $\va{v}$ with components $\class{pink}{x}$, $\class{pink}{y}$, and $\class{pink}{z}$ in the standard basis $\class{red}{\vu{x}}$, $\class{red}{\vu{y}}$, and $\class{red}{\vu{z}}$.
Then, $\va{v}$ can be written as $\va{v} = \class{pink}{x} \class{red}{\vu{x}} + \class{pink}{y} \class{red}{\vu{y}} + \class{pink}{z} \class{red}{\vu{z}}$.

The key insight is that we can write this as the product of a row vector and a column vector, where the row vector contains the basis vectors and the column vector contains the components:

$$
\begin{equation}
\va{v} = \mqty[\class{red}{\vu{x}} & \class{red}{\vu{y}} & \class{red}{\vu{z}}] \mqty[\class{pink}{x} \\ \class{pink}{y} \\ \class{pink}{z}]
\end{equation}
$$

Next, suppose a new basis $\class{darkblue}{\tilde{\vu{x}}}$, $\class{darkblue}{\tilde{\vu{y}}}$, and $\class{darkblue}{\tilde{\vu{z}}}$ is given and represented by some $3 \times 3$ matrix $\vb{T}$.
Then, we can write the new basis row vector as the following:

$$
\begin{equation}
\mqty[\class{darkblue}{\tilde{\vu{x}}} & \class{darkblue}{\tilde{\vu{y}}} & \class{darkblue}{\tilde{\vu{z}}}] = \mqty[\class{red}{\vu{x}} & \class{red}{\vu{y}} & \class{red}{\vu{z}}] \vb{T}
\end{equation}
$$

We want to be able to write $\va{v}$ in terms of the new basis:

$$
\begin{equation}
\va{v} = \mqty[\class{darkblue}{\tilde{\vu{x}}} & \class{darkblue}{\tilde{\vu{y}}} & \class{darkblue}{\tilde{\vu{z}}}] \mqty[\class{blue}{\tilde{x}} \\ \class{blue}{\tilde{y}} \\ \class{blue}{\tilde{z}}]
\end{equation}
$$

The key insight is that we can start from the original expansion and add anything in the middle that is equal to the identity matrix.
For instance, we can write $\vb{T} \vb{T}^{-1} = I$:

$$
\begin{equation}
\begin{split}
\va{v} &= \mqty[\class{red}{\vu{x}} & \class{red}{\vu{y}} & \class{red}{\vu{z}}] \mqty[\class{pink}{x} \\ \class{pink}{y} \\ \class{pink}{z}] \\
&= \mqty[\class{red}{\vu{x}} & \class{red}{\vu{y}} & \class{red}{\vu{z}}] \vb{I} \mqty[\class{pink}{x} \\ \class{pink}{y} \\ \class{pink}{z}] \\
&= \underbrace{\mqty[\class{red}{\vu{x}} & \class{red}{\vu{y}} & \class{red}{\vu{z}}] \vb{T}} \vb{T}^{-1} \mqty[\class{pink}{x} \\ \class{pink}{y} \\ \class{pink}{z}] \\
&= \mqty[\class{darkblue}{\tilde{\vu{x}}} & \class{darkblue}{\tilde{\vu{y}}} & \class{darkblue}{\tilde{\vu{z}}}] \vb{T}^{-1} \mqty[\class{pink}{x} \\ \class{pink}{y} \\ \class{pink}{z}] \\
&= \mqty[\class{darkblue}{\tilde{\vu{x}}} & \class{darkblue}{\tilde{\vu{y}}} & \class{darkblue}{\tilde{\vu{z}}}] \mqty[\class{blue}{\tilde{x}} \\ \class{blue}{\tilde{y}} \\ \class{blue}{\tilde{z}}]
\end{split}
\end{equation}
$$

As such, we can see that:

$$
\begin{equation}
\mqty[\class{blue}{\tilde{x}} \\ \class{blue}{\tilde{y}} \\ \class{blue}{\tilde{z}}] = \vb{T}^{-1} \mqty[\class{pink}{x} \\ \class{pink}{y} \\ \class{pink}{z}]
\end{equation}
$$

Going back to quantum mechanics, the principle is the same. We can write the components of a vector in the new basis as the inverse of the transformation matrix times the components in the old basis.
But because the transformation matrix $U$ is unitary, that is the same as taking the adjoint of the transformation matrix.
Then:

$$
\begin{equation}
\mqty[\braket{b_1}{\psi} \\ \braket{b_2}{\psi} \\ \vdots \\ \braket{b_n}{\psi}] = U^\dagger \mqty[\braket{a_1}{\psi} \\ \braket{a_2}{\psi} \\ \vdots \\ \braket{a_n}{\psi}]
\end{equation}
$$

The fact that basis vectors are covariant (transform with the forward transformation $U$) and components are contravariant (transform with the inverse transformation $U^\dagger$) is why the vector itself $\ket{\psi}$ is **invariant** under a change of basis.
Intuitively, it makes sense that the components and basis "cancel" each other out.
