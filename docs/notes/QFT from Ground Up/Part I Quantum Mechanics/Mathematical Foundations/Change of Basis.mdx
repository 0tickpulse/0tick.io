---
sidebar_position: 6
---

import { Mafs, Coordinates, Plot, Line, Circle, Theme, useMovablePoint, useStopwatch, vec, Vector, LaTeX, Polygon, Transform } from "mafs";

import { useState, useCallback } from "react";
import { lineLabel } from "@site/src/utilities/lines";
import { color } from "@site/src/utilities/colors"
import TOCInline from '@theme/TOCInline';
import * as MB from "mathbox-react"
import * as THREE from "three"
import { OrbitControls } from "three/examples/jsm/controls/OrbitControls"

# Change of Basis

There are many cases in quantum mechanics where we need to change the basis of a vector from one basis to another.
For example, in our discussion of spin states, we can use $\ket{S_z; +}$ and $\ket{S_z; -}$ as the basis vectors, but we can also use $\ket{S_x; +}$ and $\ket{S_x; -}$ as the basis vectors.

The change of basis is very important in many areas of physics.
For instance, the entirety of special relativity just boils down to changing the basis of spacetime vectors.
We now explore some mathematical concepts that will help us understand how to change the basis of a vector.

In this page, we shall employ a color scheme to help us distinguish between different bases:

- The old basis is dark red, and the new basis is dark blue.
- The old components are pink, and the new components are light blue.

## Table of Contents

<TOCInline toc={toc} />

## The Transformation Matrix

Suppose $\class{red}{\ket{a_1}}, \class{red}{\ket{a_2}}, \ldots, \class{red}{\ket{a_n}}$ and $\class{blue}{\ket{b_1}}, \class{blue}{\ket{b_2}}, \ldots, \class{blue}{\ket{b_n}}$ are two sets of orthonormal basis vectors.
In order to transform one basis to another, we need to find an operator that can transform the basis vectors.

Denote this operator as $\hat{U}$, such that $U\class{red}{\ket{a_i}} = \class{blue}{\ket{b_i}}$.
In order to find out what this operator is, we can use a neat trick using the outer product.

Consider the following: $\dyad{b_i}{a_i} \class{red}{\ket{a_j}}$.
When $i = j$, this is just $\class{blue}{\ket{b_i}} \braket{a_i} = \class{blue}{\ket{b_i}}$.
When $i \neq j$, this is $\class{blue}{\ket{b_i}} \braket{a_i}{a_j} = 0$.
Therefore, we can write $\hat{U}$ as a sum of all of these outer products:

$$
\begin{equation}
U = \sum_i \dyad{b_i}{a_i}
\end{equation}
$$

### Unitarity

Previously, we have seen that a Hermitian operator $A$ satisfies $A^\dagger = A$.
Let's see what happens when we take the adjoint of $\hat{U}$, remembering the rules of adjoints:

$$
\begin{equation}
\begin{split}
\hat{U}^\dagger &= \qty(\sum_i \dyad{b_i}{a_i})^\dagger \\
&= \sum_i \qty(\dyad{b_i}{a_i})^\dagger \\
&= \sum_i \dyad{a_i}{b_i}
\end{split}
\end{equation}
$$

Notice what happens when we multiply $\hat{U}$ and $U^\dagger$:

$$
\begin{equation}
\begin{split}
\hat{U}^\dagger \hat{U} &= \qty(\sum_i \dyad{a_i}{b_i}) \qty(\sum_{j = 1}^{n} \dyad{b_j}{a_j}) \\
&= \sum_i \sum_{j = 1}^{n} \dyad{a_i}{b_i} \dyad{b_j}{a_j} \\
&= \sum_i \sum_{j = 1}^{n} \dyad{a_i}{a_i} \delta_{ij}
\end{split}
\end{equation}
$$

Since $\delta_{ij}$ ensures that the sum is only nonzero when $i = j$, we can remove one of the sums:

$$
\begin{equation}
\begin{split}
\hat{U}^\dagger \hat{U} &= \sum_i \dyad{a_i}{a_i} \\
&= I
\end{split}
\end{equation}
$$

where we have used the completeness relation $\sum_i \dyad{a_i}{a_i} = I$.
Therefore, $\hat{U}^\dagger \hat{U} = I$. We call such an operator $\hat{U}$ an **unitary operator**.
Unitary operators will be very important when we discuss the time-evolution of quantum states.

We can summarize what we have found in the following theorem:

<Boxed>
Given two sets of orthonormal and complete basis vectors $\class{red}{\ket{a_i}}$ and $\class{blue}{\ket{b_i}}$, there always exists a unitary operator $\hat{U}$ that transforms one basis to another such that $\hat{U}\class{red}{\ket{a_i}} = \class{blue}{\ket{b_i}}$.
</Boxed>

### Matrix Representation

As always, we can represent operators as matrices, and the change of basis operator $\hat{U}$ is no exception.
Recall that the $ij$-th element of the matrix representation of an operator is $\mel{E_i}{\hat{A}}{E_j}$.
Therefore, the matrix representation of $\hat{U}$ is:

$$
\begin{equation}
U_{ij} = \mel{a_i}{\hat{U}}{a_j} = \braket{b_i}{a_j}
\end{equation}
$$

It is insightful to see that this is the same as the transformation matrix we have seen in linear algebra:

$$
\begin{equation}
T = \mqty[\vu{x} \cdot \tilde{\vu{x}} & \vu{x} \cdot \tilde{\vu{y}} & \vu{x} \cdot \tilde{\vu{z}} \\ \vu{y} \cdot \tilde{\vu{x}} & \vu{y} \cdot \tilde{\vu{y}} & \vu{y} \cdot \tilde{\vu{z}} \\ \vu{z} \cdot \tilde{\vu{x}} & \vu{z} \cdot \tilde{\vu{y}} & \vu{z} \cdot \tilde{\vu{z}}]
\end{equation}
$$

where $\vu{x}, \vu{y}, \vu{z}$ and $\tilde{\vu{x}}, \tilde{\vu{y}}, \tilde{\vu{z}}$ are two sets of orthonormal basis vectors.
In the case of curvilinear coordinates, we can use a combination of the metric tensor and the Jacobian to find the transformation matrix.

### Components of Vectors Under Change of Basis

The next question we might ask is: how do the components of a vector change under a change of basis?
If one is familiar with tensors, one might recall that vectors have contravariant components; the components transform in the opposite way to the basis vectors.

It is easy to see why this is the case.
I borrow from Eigenchris on YouTube for this explanation.
Consider a vector $\ket{\psi}$ with components $\class{pink}{\braket{a_1}{\psi}}$, $\class{pink}{\braket{a_2}{\psi}}$, and $\class{pink}{\braket{a_3}{\psi}}$ in the standard basis $\class{red}{\ket{a_1}}$, $\class{red}{\ket{a_2}}$, and $\class{red}{\ket{a_3}}$.
Then, $\ket{\psi}$ can be written as $\ket{\psi} = \class{pink}{\braket{a_1}{\psi}} \class{red}{\ket{a_1}} + \class{pink}{\braket{a_2}{\psi}} \class{red}{\ket{a_2}} + \class{pink}{\braket{a_3}{\psi}} \class{red}{\ket{a_3}}$.

The key insight is that we can write this as the product of a row vector and a column vector, where the row vector contains the basis vectors and the column vector contains the components:

$$
\begin{equation}
\ket{\psi} = \mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \class{red}{\ket{a_3}}] \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \class{pink}{\braket{a_3}{\psi}}]
\end{equation}
$$

Next, suppose a new basis $\class{darkblue}{\ket{b_1}}$, $\class{darkblue}{\ket{b_2}}$, and $\class{darkblue}{\ket{b_3}}$ is given and represented by some $3 \times 3$ matrix $U$.
Then, we can write the new basis row vector as the following:

$$
\begin{equation}
\mqty[\class{darkblue}{\ket{b_1}} & \class{darkblue}{\ket{b_2}} & \class{darkblue}{\ket{b_3}}] = \mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \class{red}{\ket{a_3}}] U
\end{equation}
$$

We want to be able to write $\ket{\psi}$ in terms of the new basis:

$$
\begin{equation}
\ket{\psi} = \mqty[\class{darkblue}{\ket{b_1}} & \class{darkblue}{\ket{b_2}} & \class{darkblue}{\ket{b_3}}] \mqty[\class{blue}{\braket{b_1}{\psi}} \\ \class{blue}{\braket{b_2}{\psi}} \\ \class{blue}{\braket{b_3}{\psi}}]
\end{equation}
$$

The key insight is that we can start from the original expansion and add anything in the middle that is equal to the identity matrix.
For instance, we can write $U U^{-1} = I$:

$$
\begin{equation}
\begin{split}
\ket{\psi} &= \mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \class{red}{\ket{a_3}}] \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \class{pink}{\braket{a_3}{\psi}}] \\
&= \mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \class{red}{\ket{a_3}}] I \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \class{pink}{\braket{a_3}{\psi}}] \\
&= \underbrace{\mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \class{red}{\ket{a_3}}] U} U^{-1} \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \class{pink}{\braket{a_3}{\psi}}] \\
&= \mqty[\class{darkblue}{\ket{b_1}} & \class{darkblue}{\ket{b_2}} & \class{darkblue}{\ket{b_3}}] U^{-1} \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \class{pink}{\braket{a_3}{\psi}}] \\
&= \mqty[\class{darkblue}{\ket{b_1}} & \class{darkblue}{\ket{b_2}} & \class{darkblue}{\ket{b_3}}] \mqty[\class{blue}{\braket{b_1}{\psi}} \\ \class{blue}{\braket{b_2}{\psi}} \\ \class{blue}{\braket{b_3}{\psi}}]
\end{split}
\end{equation}
$$

As such, we can see that:

$$
\begin{equation}
\mqty[\class{blue}{\braket{b_1}{\psi}} \\ \class{blue}{\braket{b_2}{\psi}} \\ \class{blue}{\braket{b_3}{\psi}}] = U^{-1} \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \class{pink}{\braket{a_3}{\psi}}]
\end{equation}
$$

But because the transformation matrix $U$ is unitary, that is the same as taking the adjoint of the transformation matrix.
Then:

$$
\begin{equation}
\mqty[\class{blue}{\braket{b_1}{\psi}} \\ \class{blue}{\braket{b_2}{\psi}} \\ \vdots \\ \class{blue}{\braket{b_n}{\psi}}] = U^\dagger \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \vdots \\ \class{pink}{\braket{a_n}{\psi}}]
\end{equation}
$$

The fact that basis vectors are covariant (transform with the forward transformation $U$) and components are contravariant (transform with the inverse transformation $U^\dagger$) is why the vector itself $\ket{\psi}$ is **invariant** under a change of basis.
Intuitively, it makes sense that the components and basis "cancel" each other out.

### Components of Operators Under Change of Basis

The components of operators also transform under a change of basis.

Suppose we have a state vector $\ket{\psi}$ and an operator $\hat{A}$.
The vector's components are $\class{pink}{\braket{a_i}{\psi}}$ in the old basis $\class{red}{\ket{a_i}}$ and $\class{blue}{\braket{b_i}{\psi}}$ in the new basis $\class{blue}{\ket{b_i}}$.
Recall that operators act on state vectors to produce new state vectors.
As such, to act on $\ket{\psi}$ in $\class{blue}{\ket{b_i}}$, we can simply transform $\ket{\psi}$ to the old basis $\class{red}{\ket{a_i}}$, act with the operator $\hat{A}$, and then transform back to the new basis $\class{blue}{\ket{b_i}}$.
Thus, the operator in the new basis is equal to $U^\dagger \hat{A} U$.

To see this more clearly, recall that $\ket{\psi}$ can be written as a row-vector times a column-vector:

$$
\begin{equation}
\ket{\psi} = \mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \ldots & \class{red}{\ket{a_n}}] \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \vdots \\ \class{pink}{\braket{a_n}{\psi}}]
\end{equation}
$$

Performing an operator $\hat{A}$ is equivalent to multiplying its matrix representation by the column-vector:

$$
\begin{equation}
\hat{A}\ket{\psi} = \mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \ldots & \class{red}{\ket{a_n}}] A \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \vdots \\ \class{pink}{\braket{a_n}{\psi}}]
\end{equation}
$$

As we know, the basis vectors transform with the forward transformation $U$ and the components transform with the inverse transformation $U^\dagger$.
As such, we can write that:

$$
\begin{equation}
\begin{split}
\hat{A}\ket{\psi} &= \mqty[\class{red}{\ket{a_1}} & \class{red}{\ket{a_2}} & \ldots & \class{red}{\ket{a_n}}] A \mqty[\class{pink}{\braket{a_1}{\psi}} \\ \class{pink}{\braket{a_2}{\psi}} \\ \vdots \\ \class{pink}{\braket{a_n}{\psi}}] \\
&= \mqty[\class{blue}{\ket{b_1}} & \class{blue}{\ket{b_2}} & \ldots & \class{blue}{\ket{b_n}}] U^\dagger A U \mqty[\class{blue}{\braket{b_1}{\psi}} \\ \class{blue}{\braket{b_2}{\psi}} \\ \vdots \\ \class{blue}{\braket{b_n}{\psi}}]
\end{split}
\end{equation}
$$

Notice that the part in the middle, $U^\dagger A U$, must be the matrix representation of the operator $\hat{A}$ in the new basis.
The transformation $U^\dagger A U$ is called the **similarity transformation** of the operator $\hat{A}$.

:::info Linear map transformations using tensors

In the language of multilinear algebra, a linear map is a $(1, 1)$ tensor, meaning it has one contravariant index and one covariant index.
A multilinear map can generally be represented as $A = \class{pink}{A^i_j} \class{red}{\vb{e}_i} \otimes \class{red}{\vb{e}^j}$.
$\class{red}{\vb{e}_i}$ and $\class{red}{\vb{e}^j}$ are the basis vectors and linear functionals/covectors, corresponding to $\class{red}{\ket{a_i}}$ and $\class{red}{\bra{a_j}}$ in Dirac notation.
$\otimes$ is the tensor product, similar to the outer product that we have used.

All of this is just a different notation for what we have already seen.
However, the tensor notation make it very easy to see how the components of a tensor transform under a change of basis.
Applying a change of basis to both bases in the expression for $A$ yields:

$$
\begin{equation}
A = \class{pink}{A^i_j} \class{red}{\vb{e}_i} \otimes \class{red}{\vb{e}^j} = \class{pink}{A^i_j} \qty[\qty(U^\dagger)^k_i \class{blue}{\tilde{\vb{e}}_k}] \otimes \qty[\qty(U)^j_l \class{blue}{\tilde{\vb{e}}^l}] = \qty[\qty(U^\dagger)^k_i \class{pink}{A^i_j} U^j_l] \class{blue}{\tilde{\vb{e}}_k} \otimes \class{blue}{\tilde{\vb{e}}^l} = \class{blue}{\tilde{A}^k_l} \class{blue}{\tilde{\vb{e}}_k} \otimes \class{blue}{\tilde{\vb{e}}^l}
\end{equation}
$$

Thus we see that $\class{blue}{\tilde{A}^k_l} = \qty(U^\dagger)^k_i \class{pink}{A^i_j} U^j_l$, and hence $\class{blue}{\tilde{A}} = U^\dagger A U$.
This is the same as the similarity transformation we have seen before.

:::

## Trace

The trace of an operator is defined as the sum of the diagonal elements of the matrix representation of the operator.
In the context of quantum mechanics, the trace of an operator is very important because it is invariant under a change of basis.

Suppose we have an operator $\hat{A}$ with matrix representation $A$.
Then, the trace of the operator is:

$$
\begin{equation}
\Tr(A) = \sum_i A_{ii} = \sum_i \mel{a_i}{\hat{A}}{a_i}
\end{equation}
$$

Now, suppose we change the basis from $\class{red}{\ket{a_i}}$ to $\class{blue}{\ket{b_i}}$.
To show that the trace is invariant under a change of basis, we can use a few completeness relations:

$$
\begin{equation}
\begin{split}
\Tr(A) &= \sum_i \mel{a_i}{\hat{A}}{a_i} \\
&= \sum_i \class{yellow}{\sum_j} \class{green}{\sum_k} \bra{a_i} \class{yellow}{\dyad{b_j}} \hat{A} \class{green}{\dyad{b_k}} \ket{a_i} \\
&= \sum_i \sum_j \sum_k \braket{b_j}{a_i} \mel{b_j}{\hat{A}}{b_k} \braket{b_k}{a_i} \\
&= \sum_i \sum_j \sum_k \delta_{ij} \mel{b_j}{\hat{A}}{b_k} \delta_{ik} \\
&= \sum_i \sum_k \mel{b_i}{\hat{A}}{b_k} \delta_{ik} \\
&= \sum_i \mel{b_i}{\hat{A}}{b_i}
\end{split}
\end{equation}
$$

Therefore, the trace of an operator is invariant under a change of basis.

### Properties of the Trace

The trace of an operator has a few properties that are useful to know.

1. **Linearity**: $\Tr(A + B) = \Tr(A) + \Tr(B)$.

    A simple proof of this is to write the matrix representation of $A + B$ and sum the diagonal elements.

2. **Cyclic Property**: $\Tr(ABC) = \Tr(BCA) = \Tr(CAB)$.

    This is a bit more complicated to prove, but it is a useful property to know.

3. **Invariance Under Change of Basis**: As we have shown, the trace of an operator is invariant under a change of basis.

We can also show what happens to a few other quantities with a trace:

1. **Trace of the Similarity Transformation**: $\Tr(U^\dagger A U) = \Tr(A)$.

    This is a direct consequence of the invariance of the trace under a change of basis.
2. **Trace of an outer product**: $\Tr(\dyad{a_i}{b_j}) = \braket{b_j}{a_i}$.

    This is because $\Tr(\dyad{a_i}{b_j}) = \sum_k \mel{a_k}{\dyad{a_i}{b_j}}{a_k} = \sum_k \delta_{ik} \braket{b_j}{a_k} = \braket{b_j}{a_i}$.

## Diagonalization and Spectral Decomposition

Under a change of basis, we should be able to find a basis in which the operator is diagonal.
This is a very powerful tool because diagonal operators are very easy to work with.
Since the diagonal elements of a diagonal operator are the eigenvalues, we can diagonalize an operator by finding its eigenvalues and eigenvectors.

Given an operator $\hat{A}$, any eigenvector $\ket{a_i}$ of $\hat{A}$ satisfies the eigenvalue equation $\hat{A}\ket{a_i} = a_i\ket{a_i}$.
This can then be rewritten as $A\ket{a_i} - a_i\ket{a_i} = 0$, or $(A - a_i I)\ket{a_i} = 0$.
In other words, the operator $A - a_i I$ has a nontrivial null space, which is spanned by the eigenvector $\ket{a_i}$.
For $A - a_i I$ to have a nontrivial null space, it must be singular, which means that $\det(A - a_i I) = 0$.
Of course, this is the characteristic equation of the operator.

The characteristic equation is a polynomial of degree $n$ in $a_i$.
If an operator has $n$ distinct eigenvalues, then it can be diagonalized by making the eigenvectors the basis vectors.
Otherwise, it is not diagonalizable.

Regarding diagonalization, we also have an important theorem:

> **Theorem**: Let $A$ be an operator with non-degenerate eigenvalues (i.e. all eigenvalues correspond to a single eigenvector).
> If $A$ and $B$ commute, then $B$ is diagonal in the basis of eigenvectors of $A$.

**Proof**: We already know that $A$ is diagonal in the basis of eigenvectors of $A$ (by definition).

We also know that if $A$ and $B$ commute, then they share a common set of eigenvectors.
Thus, $B$ is diagonal in the basis of eigenvectors of $A$ because they are also the eigenvectors of $B$. $\blacksquare$

### Spectral Decomposition

The spectral decomposition of an operator is a way to express the operator in terms of its eigenvalues and eigenvectors.

Given an operator $\hat{A}$ with eigenvectors $\ket{a_i}$ and eigenvalues $a_i$, we can write the operator as:

$$
\begin{equation}
\hat{A} = \sum_i a_i \dyad{a_i}{a_i}
\end{equation}
$$

## Appendix: Proof of the Cyclic Property of the Trace

The cyclic property of the trace is a bit more complicated to prove.
We shall prove that $\Tr(ABC) = \Tr(BCA)$, and the proof for the other cyclic properties follows similarly.

We will use two different notations to prove this property: Dirac notation and tensor notation.
Eventually, once we start studying relativstic quantum mechanics, we will need a solid grasp of both notations, so it is good to be familiar with both.

### Proof with Dirac Notation

First, start by expanding the trace of $ABC$ and applying the completeness relation twice:

$$
\begin{equation}
\begin{split}
\Tr(ABC) &= \sum_i \mel{a_i}{ABC}{a_i} \\
&= \sum_i \class{blue}{\sum_j} \class{green}{\sum_k} \mel{a_i}{A \class{blue}{\dyad{a_j}{a_j}} B \class{green}{\dyad{a_k}{a_k}} C}{a_i} \\
\end{split}
\end{equation}
$$

We can re-interpret the expression inside the sum as the product of three numbers: $\mel{a_i}{A}{a_j}$, $\mel{a_j}{B}{a_k}$, and $\mel{a_k}{C}{a_i}$.
Hence, because complex multiplication is commutative, we can rearrange the terms in the sum:

$$
\begin{equation}
\begin{split}
\Tr(ABC) &= \sum_i \sum_j \sum_k \mel{a_i}{A}{a_j} \mel{a_j}{B}{a_k} \mel{a_k}{C}{a_i} \\
&= \sum_j \sum_k \sum_i \mel{a_j}{B}{a_k} \mel{a_k}{C}{a_i} \mel{a_i}{A}{a_j}
\end{split}
\end{equation}
$$

Now we re-interpret the expression in the sum back as a big matrix element with projection operators in the middle:

$$
\begin{equation}
\Tr(ABC) = \sum_j \class{green}{\sum_k} \class{blue}{\sum_i} \mel{a_j}{B \class{green}{\dyad{a_k}{a_k}} C \class{blue}{\dyad{a_i}{a_i}} A}{a_j}
\end{equation}
$$

Now, because $\sum_i \dyad{a_i}{a_i} = I$, we can remove the sums over $k$ and $i$, and remove the projection operators:

$$
\begin{equation}
\Tr(ABC) = \sum_j \mel{a_j}{B C A}{a_j} = \Tr(BCA)
\end{equation}
$$

Thus we have shown that $\Tr(ABC) = \Tr(BCA)$.

### Proof with Tensor Notation

We can also prove the cyclic property using tensor notation.
Recall that the matrix representation of an operator is a $(1, 1)$ tensor, and the matrix representation of the operator $ABC$ is a $(1, 1)$ tensor with three indices.

Let $X$ be the matrix representation of $ABC$ and $Y$ be the matrix representation of $BCA$.

Then, the $ij$-th element of $X$ is $X^i_j = A^i_k B^k_l C^l_j$.
We know that this is the specific representation of $ABC$ in that order from a heuristic: notice that the indices connect diagonally across the three matrices:

$$
\begin{equation}
X^i_j = A^i_{\class{red}{k}} B^{\class{red}{k}}_{\class{green}{l}} C^{\class{green}{l}}_j
\end{equation}
$$

The indices that are not paired become the indices of the resulting matrix.
The cyclic property of the trace is then:

The trace of $ABC$ is then:

$$
\begin{equation}
\Tr(ABC) = X^i_i = A^i_k B^k_l C^l_i
\end{equation}
$$

Notice that we can reorder these terms on the right while retaining the diagonal connection of the indices:

$$
\begin{equation}
\Tr(ABC) = B^k_{\class{red}{l}} C^{\class{red}{l}}_{\class{green}{i}} A^{\class{green}{i}}_k = Y^k_k = \Tr(BCA)
\end{equation}
$$

Thus we have shown that $\Tr(ABC) = \Tr(BCA)$.

The advantage of tensor notation is that it is very easy to see how the indices connect across the matrices, and how the cyclic property of the trace is a direct consequence of the connection of the indices.
Einstein's summation convention also helps to make everything more concise.
In fact, we can write this entire proof in a single line:

$$
\begin{equation}
\Tr(ABC) = A^i_k B^k_l C^l_i = B^k_l C^l_i A^i_k = \Tr(BCA)
\end{equation}
$$
