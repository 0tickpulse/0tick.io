---
sidebar_position: 2
---

import { Mafs, Coordinates, Plot, Line, Circle, Theme, useMovablePoint, useStopwatch, vec, Vector, LaTeX, Polygon, Transform } from "mafs";

import { useState, useCallback } from "react";
import { lineLabel } from "@site/src/utilities/lines";
import { color } from "@site/src/utilities/colors"
import TOCInline from '@theme/TOCInline';
import * as MB from "mathbox-react"
import * as THREE from "three"
import { OrbitControls } from "three/examples/jsm/controls/OrbitControls"

# Observables and Operators

In quantum mechanics, we are interested in the properties of particles, such as their position, momentum, energy, and so on.
In the previous page, we dealt with the abstract state of a particle, represented by a state vector $\ket{\psi}$.
Now, we will discuss how we can measure these properties and how they are represented mathematically.

## Table of Contents

<TOCInline toc={toc} />

## Observables

In quantum mechanics, an **observable** is a physical quantity that can be measured.
These are properties of a particle that we can observe, such as position $x$, momentum $p$, energy $E$, and so on.
Observables are repesented as linear operators acting on the state vector of the particle.
These operators are denoted by a hat, such as $\hat{x}$, $\hat{p}$, $\hat{E}$, and so on.

Suppose we have an observable $\hat{E}$ for the energy of a particle.
How can we find out which energies are possible for the particle?
These determine the basis of the space in which the state vector of the particle lies.

When we measure the energy, suppose we get a value $E_1$.
At that moment, the state vector of the particle collapses to the state vector $\ket{E_1}$.
These states are called **definite states** because they represent a definite value of the observable, not a superposition of values.

For all energies, then, each energy $E_i$ corresponds to a definite state $\ket{E_i}$.
It turns out that $E_i$ is an eigenvalue of the operator $\hat{E}$, and $\ket{E_i}$ is the corresponding eigenvector, known as an **eigenstate**.

### Properties of Observables

First, **observables have real eigenvalues**. This is to prevent observables (like energy) from being imaginary, which is not physically meaningful.

Second, **eigenvectors must span the entire space**. This ensures that any state vector can be expressed as a linear combination of the eigenvectors.
If the eigenvectors do not span the space, then there are states that cannot be represented by the observable.
But you cannot have a particle with "none" energy or "none" momentum; it must have some value. Hence, the eigenvectors must span the space.

Third, **eigenvectors must be orthogonal**. This ensures that the eigenvectors are independent of each other.
If not, then one eigenvector can be expressed as a linear combination of the others, which means that the observable is a superposition, not a definite state, contradicting our principles.

In the future, we will show that these properties make observables something known as **Hermitian operators**.

## Born Rule

When we measure an observable, we get a definite value, not a superposition of values.
The probability of getting a particular value $E_i$ is given by the **Born rule**.
It is technically a postulate of quantum mechanics (a postulate is a statement that is assumed to be true without proof), but we can see how it can be derived from physical intuitions.
(We will take a similar approach when discussing the SchrÃ¶dinger equation, which is another postulate, but still has a physical basis.
After all, in order to come up with postulates, physicists can't just make up random rules; they must be based on experimental observations.)

Suppose we have a state vector $\ket{\psi}$, and for simplicity, consider a two-dimensional vector space.
This means that the state vector can be expressed as a linear combination of two basis vectors $\ket{E_1}$ and $\ket{E_2}$:

$$
\begin{equation}
\ket{\psi} = c_1 \ket{E_1} + c_2 \ket{E_2}
\end{equation}
$$

Assuming $\ket{E_1}$ and $\ket{E_2}$ are orthonormal, we can see them on a plane.
One intuition is that for a state vector $\ket{\psi}$, the probability of measuring $E_1$ is higher if the vector is closer to $\ket{E_1}$.

{(() => {
    const psi = useMovablePoint([Math.sqrt(2) / 2, Math.sqrt(2) / 2], { constrain: ([x, y]) => vec.normalize([x, y]) });
    const E1_psi = psi.point[0];
    const E2_psi = psi.point[1];
    return (
        <div className="card">
            <div className="card__body">
                <Mafs viewBox={{ y: [-1, 1] }}>
                    <Coordinates.Cartesian xAxis={{ labels: () => "" }} yAxis={{ labels: () => "" }} />
                    <Vector tip={[1, 0]} color={color("blue")} />
                    <LaTeX tex="|E_1\rangle" at={[1.2, 0]} color={color("blue")} />
                    <Vector tip={[0, 1]} color={color("blue")} />
                    <LaTeX tex="|E_2\rangle" at={[0, 1.2]} color={color("blue")} />

                    <Vector tip={psi.point} color={color("yellow")} />
                    <LaTeX tex="|\psi\rangle" at={vec.withMag(psi.point, 1.2)} color={color("yellow")} />
                    {psi.element}

                    <Polyline points={[[0, 0], [psi.point[0], 0], psi.point]} strokeStyle="dashed" />
                    <LaTeX tex={psi.point[1].toFixed(2)} at={[psi.point[0] + (psi.point[0] > 0 ? 0.2 : -0.2), psi.point[1] / 2]} />
                    <LaTeX tex={psi.point[0].toFixed(2)} at={[psi.point[0] / 2, (psi.point[1] > 0 ? -0.2 : 0.2)]} />
                </Mafs>
            </div>
            <div className="card__footer">
                <p>
                    Horizontal component: <MathEquation inline>{"\\ket{\\psi}_x =" + E1_psi.toFixed(2)}</MathEquation>
                    <br />
                    Vertical component: <MathEquation inline>{"\\ket{\\psi}_y =" + E2_psi.toFixed(2)}</MathEquation>
                </p>
            </div>
        </div>
    )
})()}

To get the component of $\ket{\psi}$ along $\ket{E_i}$, we take the inner product of the two vectors:

$$
\begin{equation}
(\text{Component along }\ket{E_i}) = \braket{E_i}{\psi} = c_i
\end{equation}
$$

One guess is that the probability of measuring $E_i$ is the magnitude of the component $c_i$:

$$
\begin{equation}
P(E_i) \overset{?}{=} |c_i| \label{eq:prob_guess}
\end{equation}
$$

But this is not quite correct.
Another guess would be that the probability is the square of the magnitude of the component.
When we drag the state vector along a circle, the sum of squares of the components is always $1$.
This would make sense because the sum of probabilities of all possible outcomes must be $1$.
Of course, this doesn't actually prove the Born rule, but it gives us an intuition behind it.
Instead, let's see why the guess $P(E_i) = |c_i|$ is incorrect.

### Flaws in the Guess

Let's lay down what we know so far, and what we need from the probability $P(E_i)$:

1. The state vector $\ket{\psi}$ can be expressed as a linear combination of the basis vectors $\ket{E_1}$ and $\ket{E_2}$:

    $$
    \begin{equation}
    \ket{\psi} = c_1 \ket{E_1} + c_2 \ket{E_2}
    \end{equation}
    $$

2. The probability of measuring $E_i$ is guessed to be:

    $$
    \begin{equation}
    P(E_i) = |c_i| \tag{\ref{eq:prob_guess}}
    \end{equation}
    $$

3. All probabilities must sum to $1$:

    $$
    \begin{equation}
    P(E_1) + P(E_2) = |c_1| + |c_2| = 1
    \end{equation}
    $$

The problem with the guess is that it fails the sum condition under a change of coordinates.
Suppose we have the following setup:

<Mafs viewBox={{ y: [-1, 1] }}>
    <Coordinates.Cartesian xAxis={{ labels: () => "" }} yAxis={{ labels: () => "" }} />
    <Vector tip={[Math.sqrt(2) / 2, Math.sqrt(2) / 2]} color={color("blue")} />
    <LaTeX tex="|E_1\rangle" at={vec.withMag([Math.sqrt(2) / 2, Math.sqrt(2) / 2], 1.2)} color={color("blue")} />
    <Vector tip={[-Math.sqrt(2) / 2, Math.sqrt(2) / 2]} color={color("blue")} />
    <LaTeX tex="|E_2\rangle" at={vec.withMag([-Math.sqrt(2) / 2, Math.sqrt(2) / 2], 1.2)} color={color("blue")} />

    <Vector tip={[Math.sqrt(2) / 2, 0]} color={color("yellow")} />
    <LaTeX tex="|\psi\rangle" at={vec.withMag([Math.sqrt(2) / 2, 0], 0.85)} color={color("yellow")} />
</Mafs>

In this setup, we have an orthonormal basis $\ket{E_1}$ and $\ket{E_2}$ representing the energies of the system. The state vector $\ket{\psi}$ is a linear combination of these basis vectors:

$$
\begin{equation}
\ket{\psi} = \frac{1}{2} \ket{E_1} - \frac{1}{2} \ket{E_2}
\end{equation}
$$

The coefficients correctly sum to $1$.

Next, let $\ket{L_1}$ and $\ket{L_2}$ be a new basis that represents the angular momentum of the system:

<Mafs viewBox={{ y: [-1, 1] }}>
    <Coordinates.Cartesian xAxis={{ labels: () => "" }} yAxis={{ labels: () => "" }} />
    <Vector tip={[Math.sqrt(2) / 2, Math.sqrt(2) / 2]} color={color("blue")} />
    <LaTeX tex="|E_1\rangle" at={vec.withMag([Math.sqrt(2) / 2, Math.sqrt(2) / 2], 1.2)} color={color("blue")} />
    <Vector tip={[-Math.sqrt(2) / 2, Math.sqrt(2) / 2]} color={color("blue")} />
    <LaTeX tex="|E_2\rangle" at={vec.withMag([-Math.sqrt(2) / 2, Math.sqrt(2) / 2], 1.2)} color={color("blue")} />

    <Vector tip={vec.rotate([1, 0], Math.PI / 4 + 0.32)} color={color("red")} />
    <LaTeX tex="|L_1\rangle" at={vec.withMag(vec.rotate([1, 0], Math.PI / 4 + 0.32), 1.2)} color={color("red")} />
    <Vector tip={vec.rotate([0, 1], Math.PI / 4 + 0.32)} color={color("red")} />
    <LaTeX tex="|L_2\rangle" at={vec.withMag(vec.rotate([0, 1], Math.PI / 4 + 0.32), 1.2)} color={color("red")} />

    <Vector tip={[Math.sqrt(2) / 2, 0]} color={color("yellow")} />
    <LaTeX tex="|\psi\rangle" at={vec.withMag([Math.sqrt(2) / 2, 0], 0.85)} color={color("yellow")} />
</Mafs>

Under this new basis, the state vector $\ket{\psi}$ is now a linear combination of $\ket{L_1}$ and $\ket{L_2}$:

$$
\begin{equation}
\ket{\psi} = \frac{1}{\sqrt{10}} \ket{L_1} - \frac{2}{\sqrt{10}} \ket{L_2}
\end{equation}
$$

**These coefficients do not sum to $1$**. Contradiction!
We would need to rescale $\ket{\psi}$ under the new basis to make the coefficients sum to $1$.
But this means that depending on the quantity we measure, the state vector changes, which is not physically meaningful.

(Once again, using the square of the magnitudes of the components would work because the norm of the state vector is invariant under a change of basis.)

### How do we Solve this?

We have shown that using the magnitudes of the components as probabilities does not work because the sum of probabilities does not remain $1$ under a change of basis.
Suppose instead of the magnitude, we use an arbitrary function $f$ of the components as the probability:

$$
\begin{equation}
P(E_i) = f(c_i)
\end{equation}
$$

Our goal is to find a function $f$ such that the sum of probabilities remains $1$ under a change of basis.
Furthermore, it should keep the norm of the state vector invariant.
Mathematically, we need to find a function $f$ such that:

$$
\begin{align}
\sum_i f(c_i) &= 1 \label{eq:sum_prob} \\
\norm{\ket{\psi}}^2 = \sum_i |c_i|^2 &= k^2
\end{align}
$$

Focusing on the second equation, let's zoom in on one coefficient $c_1$.

$$
\begin{equation}
c_1^2 + c_2^2 + \ldots = k^2 \label{eq:sum_squares_first}
\end{equation}
$$

In order for the sum to equal $k^2$, $c_1^2$ must not be larger than $k^2$.
Hence, $c_1 \in [-k, k]$.

If $c_1$ *is* $k$, then all other coefficients must be $0$.
If it is not $k$ and is instead some value $A$, then we can move it to the other side to get:

$$
\begin{equation}
c_2^2 + c_3^2 + \ldots = k^2 - A^2 \label{eq:sum_squares_second}
\end{equation}
$$

This means that the sum of squares of the other coefficients must be $k^2 - A^2$ and we can apply the same logic to the next coefficient $c_2$.
$c_2$ must be within the range $\qty[-\sqrt{k^2 - A^2}, \sqrt{k^2 - A^2}]$.
Once again, if it is not at the edge (say, $B$), then we can move it again and apply the same logic to the next coefficient:

$$
\begin{align}
c_1^2 + c_2^2 + c_3^2 + \ldots &= k^2 \tag{\ref{eq:sum_squares_first}} \\
c_2^2 + c_3^2 + \ldots &= k^2 - A^2 \tag{\ref{eq:sum_squares_second}} \\
c_3^2 + \ldots &= k^2 - A^2 - B^2 \\
\vdotswithin{=} \nonumber \\
c_n^2 &= k^2 - A^2 - B^2 - C^2 - \ldots
\end{align}
$$

The key is that $c_n$ does not have a choice; it must be $\pm \sqrt{k^2 - A^2 - B^2 - C^2 - \ldots}$.
All this is to say that **the components are independent of each other**, with only their range being constrained by the other components.
This means that changing one component does not affect the others:

$$
\begin{equation}
\pdv{c_i}{c_j} = 0 \quad \text{for } i \neq j
\end{equation}
$$

Another key intuition is that the probability of measuring $E_i$ should not depend on whether $\ket{\psi}$ points in the positive or negative direction of $\ket{E_i}$.
In other words:

$$
\begin{equation}
P(E_i) = f(c_i) = f(-c_i)
\end{equation}
$$

This means that the function $f$ must be symmetric about the origin, AKA an **even function**.

Going back to the first equation ($\eqref{eq:sum_prob}$), we can isolate the last coefficient $c_n$ because it behaves differently from the others:

$$
\begin{equation}
\qty(\sum_{i = 1}^{n - 1} f(c_i)) + f(c_n) = 1
\end{equation}
$$

Recall that $c_n$ must take on the value of $\pm \sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}$.
Plugging it into the equation, the sign does not matter because $f$ is an even function:

$$
\begin{equation}
\qty(\sum_{i = 1}^{n - 1} f(c_i)) + f\qty(\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}) = 1
\end{equation}
$$

Next, take the derivative of both sides with respect to $c_1$:

$$
\begin{equation}
\pdv{c_1} \qty(\sum_{i = 1}^{n - 1} f(c_i)) + \pdv{c_1} f\qty(\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}) = 0
\end{equation}
$$

In the first term, because $c_1$ is independent of the other components, the only nonzero derivative is when $i = 1$:

$$
\begin{equation}
\dv{f(c_1)}{c_1} + \pdv{c_1} f\qty(\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}) = 0
\end{equation}
$$

Applying the chain rule to the second term:

$$
\begin{equation}
\dv{f(c_1)}{c_1} - \frac{1}{2} \frac{1}{\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}} (2c_1) f'\qty(\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}) = 0
\end{equation}
$$

Rearranging to isolate $c_1$ yields:

$$
\begin{equation}
\frac{1}{c_1} \dv{f(c_1)}{c_1} = \frac{1}{\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}} f'\qty(\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots})
\end{equation}
$$

Of course, this is a general result for any component $c_i$.
We can write the same equation for $c_2$ as follows:

$$
\begin{equation}
\frac{1}{c_2} \dv{f(c_2)}{c_2} = \frac{1}{\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots}} f'\qty(\sqrt{k^2 - c_1^2 - c_2^2 - c_3^2 - \ldots})
\end{equation}
$$

Since both equations have equal right-hand sides, the left-hand sides must be equal as well:

$$
\begin{equation}
\frac{1}{c_1} \dv{f(c_1)}{c_1} = \frac{1}{c_2} \dv{f(c_2)}{c_2} \label{eq:separated_eq}
\end{equation}
$$

This is similar to how separation of variables works when solving a partial differential equation.
We have an equation of the form $g(c_1) = h(c_2)$. The key is that the left-hand side depends only on $c_1$ and the right-hand side depends only on $c_2$.
*If* $g(c_1)$ changed with $c_1$, then if we let $c_1$ change but keep $c_2$ constant, the left-hand side would change but the right-hand side would not.
This would violate the equality, so $g(c_1)$ must be a constant.

More explicitly, we can see this by differentiating both sides of $\eqref{eq:separated_eq}$ with respect to $c_1$:

$$
\begin{equation}
\pdv{c_1} \qty(\frac{1}{c_1} \dv{f(c_1)}{c_1}) = \pdv{c_1} \qty(\frac{1}{c_2} \dv{f(c_2)}{c_2}) = 0
\end{equation}
$$

This means that the derivative of the left-hand side with respect to $c_1$ is zero, which means that the left-hand side is a constant.

Since this applies to any $c_i$, we can rewrite the equation as:

$$
\begin{equation}
\frac{1}{c_i} \dv{f(c_i)}{c_i} = \text{constant} = \lambda
\end{equation}
$$

Rearranging:

$$
\begin{equation}
\dv{f(c_i)}{c_i} = \lambda c_i
\end{equation}
$$

Finally, integrating both sides gives us the function $f$:

$$
\begin{equation}
f(c_i) = \frac{\lambda}{2} c_i^2 + \mu \label{eq:prob_func_constants}
\end{equation}
$$

The important takeaway is that the probability function $f$ must be a quadratic function of the components.
In the next section, we find the constants $\lambda$ and $\mu$ by using some properties of $f$ that need to be satisfied.

### Solving for the Constants

First, consider what $f(0)$ should be.
In other words, what is the probability of measuring $E_i$ when the state vector has no component along $\ket{E_i}$?
It should intuitively be $0$.

Plugging this into our function ($\eqref{eq:prob_func_constants}$):

$$
\begin{equation}
f(0) = \mu = 0
\end{equation}
$$

Thus $\mu = 0$.

Next, recall that the sum of probabilities must be $1$ (equation $\eqref{eq:sum_prob}$):

$$
\begin{equation}
f(c_1) + f(c_2) + \ldots = 1 \tag{\ref{eq:sum_prob}}
\end{equation}
$$

Also remember that the sum of squares of the components must be $k^2$.
This means that if $c_1 = k$, then all other components must be $0$.
Applying this to the sum of probabilities:

$$
\begin{equation}
f(k) + f(0) + f(0) + \ldots = 1
\end{equation}
$$

We have already shown that $f(0) = 0$. Thus $f(k)$ must be $1$. Plugging this into the function:

$$
\begin{equation}
f(k) = \frac{\lambda}{2} k^2 = 1
\end{equation}
$$

This means that $\lambda = \frac{2}{k^2}$.
We have now found the exact expression for the probability function $f$. Generalizing to complex numbers (using the magnitude of the components):

$$
\begin{equation}
P(E_i) = f(c_i) = \frac{1}{k^2} |c_i|^2
\end{equation}
$$

By convention, we take $k = 1$, meaning that the magnitude of the state vector is $1$.
Hence, the probability of measuring $E_i$ is the square of the magnitude of the component $c_i$:

$$
\begin{equation}
P(E_i) = |c_i|^2
\end{equation}
$$

Finally, the coefficient can be found by taking the inner product of the state vector with the eigenvector.
Plugging this into the equation gives us the **Born rule**:

<Boxed>
**Born Rule**: The probability of measuring an observable with eigenvalue $E_i$ is:

$$
\begin{equation}
P(E_i) = |\braket{E_i}{\psi}|^2
\end{equation}
$$
</Boxed>

The above derivation is a heuristic way to understand the Born rule.
**Gleason's theorem** is a mathematical result that shows that the Born rule is the only way to assign probabilities to the outcomes of a quantum measurement.

For a discrete set of eigenvalues, the Born rule gives the probability of measuring a particular eigenvalue.
For a continuous set of eigenvalues, the Born rule gives a probability density function for the eigenvalues.
Thus, the probability of measuring an eigenvalue in a particular range is the integral of the probability density function over that range:

<Boxed>
**Born Rule for Continuous Eigenvalues**: The probability of measuring an observable with eigenvalue in the range $[a, b]$ is:

$$
\begin{equation}
P(a \leq E \leq b) = \int_a^b |\braket{E}{\psi}|^2 \dd{E}
\end{equation}
$$
</Boxed>

## Hermitian Adjoint and Hermitian Operators

Suppose we have an inner product $\braket{\phi}{\psi}$, then suppose we have an operator $\hat{A}$ acting on the state vector $\ket{\psi}$.
The inner product is then $\braket{\phi}{\hat{A} \psi}$.

Recall that there is a fundamental dual correspondence between vectors and linear functionals, or kets and bras.
The question is, is there a correspondence between the operator $\hat{A}$ and some other operator?
In other words, is there another operator $\hat{A}^\dagger$ such that $\braket{\phi}{\hat{A} \psi} = \braket{\hat{A}^\dagger \phi}{\psi}$?

This other operator, $\hat{A}^\dagger$, is known as the **Hermitian adjoint** (or simply adjoint) of $\hat{A}$.

### Properties of Hermitian Adjoint

1. Applying the Hermitian adjoint twice gives the original operator:

    $$
    \begin{equation}
    (\hat{A}^\dagger)^\dagger = \hat{A}
    \end{equation}
    $$

    To prove this, consider the inner product $\braket{\phi}{\hat{A} \psi}$.
    As we know, this is equal to $\braket{\hat{A}^\dagger \phi}{\psi}$.
    We can swap the two vectors (and take the complex conjugate) to get $\braket{\psi}{\hat{A}^\dagger \phi}^*$.
    Then, by the definition of the Hermitian adjoint again, this is equal to $\braket{(\hat{A}^\dagger)^\dagger \psi}{\phi}^*$.
    Finally, applying the conjugate symmetry again, it equals $\braket{\psi}{(\hat{A}^\dagger)^\dagger \phi}$.
    This must equal our original expression of $\braket{\phi}{\hat{A} \psi}$, so $(\hat{A}^\dagger)^\dagger = \hat{A}$. $\blacksquare$

2. The adjoint of a sum of operators is the sum of the adjoints:

    $$
    \begin{equation}
    (\hat{A} + \hat{B})^\dagger = \hat{A}^\dagger + \hat{B}^\dagger
    \end{equation}
    $$

    To prove this, consider the inner product $\braket{\phi}{(\hat{A} + \hat{B}) \psi}$:

    - By linearity of the inner product, this is equal to $\braket{\phi}{\hat{A} \psi} + \braket{\phi}{\hat{B} \psi}$.
        Then, applying the Hermitian adjoint to each term yields $\braket{\hat{A}^\dagger \phi}{\psi} + \braket{\hat{B}^\dagger \phi}{\psi}$.
        Putting the terms back together, this is equal to $\braket{(\hat{A}^\dagger + \hat{B}^\dagger) \phi}{\psi}$.
    - By the definition of the Hermitian adjoint, this is also equal to $\braket{(\hat{A} + \hat{B})^\dagger \phi}{\psi}$.

    Both expressions are equal, so $(\hat{A} + \hat{B})^\dagger = \hat{A}^\dagger + \hat{B}^\dagger$. $\blacksquare$

3. The adjoint of a product of operators is the product of the adjoints in reverse order (I will call this the product rule):

    $$
    \begin{equation}
    (\hat{A} \hat{B})^\dagger = \hat{B}^\dagger \hat{A}^\dagger
    \end{equation}
    $$

    To prove this, consider the inner product $\braket{\phi}{(\hat{A} \hat{B}) \psi}$:

    - By the definition of the Hermitian adjoint, this is equal to $\braket{(\hat{A} \hat{B})^\dagger \phi}{\psi}$.
    - But we can also apply it to each operator separately: $\braket{\phi}{(\hat{A} \hat{B}) \psi} = \braket{\hat{A}^\dagger \phi}{\hat{B} \psi} = \braket{\hat{B}^\dagger \hat{A}^\dagger \phi}{\psi}$.

    Both expressions are equal, so $(\hat{A} \hat{B})^\dagger = \hat{B}^\dagger \hat{A}^\dagger$. $\blacksquare$

4. The adjoint of a scalar is its complex conjugate:

    $$
    \begin{equation}
    c^\dagger = c^*
    \end{equation}
    $$

    This is also quite easy to prove:

    - By the linearity of the right-side and the conjugate-symmetry of the inner product: $\braket{\phi}{c \psi} = c \braket{\phi}{\psi} = \braket{c^* \phi}{\psi}$.
    - By the definition of the Hermitian adjoint, this is equal to $\braket{c^\dagger \phi}{\psi}$.

    Thus $c^\dagger = c^*$. $\blacksquare$

5. The adjoint of an operator "flips" its input and output spaces.
    This is a bit more abstract, but it is a key property of the Hermitian adjoint.
    Another way to put it is: if an operator $\hat{A}$ is defined to be $V \to W$, then its adjoint $\hat{A}^\dagger$ is defined to be $W \to V$.

    To see why this is the case, consider the inner product $\braket{\phi}{\hat{A} \psi}$, and suppose $\phi \in W$ and $\psi \in V$.
    In order for the inner product to be defined, both vectors must be in the same space. Since $\phi$ has to be in $W$, then $\hat{A} \ket{\psi}$ must be in $W$ as well.
    Thus, $\hat{A}$ must act on $\ket{\psi}$, an element in $V$ to produce a vector in $W$. Thus $\hat{A}$ is defined as $V \to W$.

    Next, using the definition of the Hermitian adjoint, $\braket{\phi}{\hat{A} \psi} = \braket{\hat{A}^\dagger \phi}{\psi}$.
    Now, $\hat{A}^\dagger \phi$ must be in $V$ because $\psi$ is in $V$. Thus, $\hat{A}^\dagger$ must act on $\phi$ to produce a vector in $V$.

    This is why the adjoint of an operator "flips" its input and output spaces.

More generally, the Hermitian adjoint is the operator that is used for the dual correspondence.
We shall now consider applying the Hermitian adjoint to a ket vector.
But wait - adjoints are applied onto *operators*, not *vectors*. What does it mean to take the adjoint of a vector?

For now, we can ignore this issue and just apply the adjoint to a vector anyway.
To see what this yields, consider taking the adjoint of an inner product: $\braket{\phi}{\psi}^\dagger$.

- Since the inner product is a scalar, its adjoint is just its complex conjugate: $\braket{\phi}{\psi}^\dagger = \braket{\phi}{\psi}^*$.
    And since the inner product is conjugate symmetric, this is just $\braket{\psi}{\phi}$.
- By the product rule of the adjoint, we can swap the order of the vectors and take the adjoint of each: $\braket{\phi}{\psi}^\dagger = \ket{\phi}^\dagger \bra{\psi}^\dagger$.

Thus we have $\braket{\phi}{\psi} = \ket{\phi}^\dagger \bra{\psi}^\dagger$.
Since this must hold for all vectors $\ket{\phi}$ and $\ket{\psi}$, we can conclude that the adjoint of a ket vector is the corresponding bra vector:

$$
\begin{equation}
\ket{\psi}^\dagger = \bra{\psi}
\end{equation}
$$

Of course, this is a bit hand-wavy, but it gives us an intuition behind why the adjoint of a vector is its corresponding bra.
The appendix contains a more rigorous proof of this fact.

### Observables and Hermitian Operators

Recall that an observable is represented by an operator $\hat{E}$.
Its eigenvectors form a basis representing the possible outcomes of the observable, and its corresponding eigenvalues are the values of the observable.
Recall the three properties of observables that we discussed earlier:

1. The eigenvalues of an observable are real (since they represent measurable quantities). In other words, $E_i \in \mathbb{R}$.
2. The eigenvectors of an observable form a complete basis (since it should be possible to measure any value of the observable). In other words, $\operatorname{span}(\ket{E_1}, \ket{E_2}, \ldots) = \mathcal{H}$.
3. The eigenvectors of an observable are orthogonal (since they represent distinct outcomes). In other words, $\braket{E_i}{E_j} = \delta_{ij}$, where $\delta_{ij}$ is the Kronecker delta.

Imagine an observable $\hat{E}$ acting on a state vector $\ket{\psi}$.
Since $\hat{E}$ has an orthonormal eigenbasis, we can expand $\ket{\psi}$ in terms of this basis:

$$
\begin{equation}
\hat{E} \ket{\psi} = \sum_i c_i \hat{E} \ket{E_i}
\end{equation}
$$

Since $\ket{E_i}$ is an eigenvector of $\hat{E}$, acting $\hat{E}$ on it just scales it by the eigenvalue $E_i$:

$$
\begin{equation}
\hat{E} \ket{\psi} = \sum_i c_i E_i \ket{E_i}
\end{equation}
$$

Next, the value for $c_i$ can be found by taking the inner product of the state vector with the eigenvector:

$$
\begin{equation}
\hat{E} \ket{\psi} = \sum_i c_i E_i \ket{E_i} = \sum_i E_i \braket{E_i}{\psi} \ket{E_i}
\end{equation}
$$

Rearranging and using the properties of Dirac notation yields:

$$
\begin{equation}
\begin{split}
\hat{E} \ket{\psi} &= \sum_i E_i \braket{E_i}{\psi} \ket{E_i} \\
&= \sum_i E_i \ket{E_i} \braket{E_i}{\psi} \\
&= \sum_i E_i \dyad{E_i} \ket{\psi} \\
&= \qty(\sum_i E_i \dyad{E_i}) \ket{\psi}
\end{split}
\end{equation}
$$

This means that the operator $\hat{E}$ can be written as:

$$
\begin{equation}
\hat{E} = \sum_i E_i \dyad{E_i} \label{eq:operator_expansion_discrete}
\end{equation}
$$

In the continuous case, such as the eigenbasis of the position operator, the sum becomes an integral:

$$
\begin{equation}
\hat{x} = \int x \dyad{x} \dd{x} \label{eq:operator_expansion_continuous}
\end{equation}
$$

For both cases, we can see what happens when we take the Hermitian adjoint of the operator.
First, since the adjoint of a sum is the sum of adjoints:

$$
\begin{equation}
\hat{E}^\dagger = \qty(\sum_i E_i \dyad{E_i})^\dagger = \sum_i \qty(E_i \dyad{E_i})^\dagger
\end{equation}
$$

By the product rule of the adjoint, this is equal to:

$$
\begin{equation}
\hat{E}^\dagger = \sum_i \qty(\bra{E_i})^\dagger \qty(E_i \ket{E_i})^\dagger = \sum_i \bra{E_i}^\dagger \ket{E_i}^\dagger E_i^\dagger
\end{equation}
$$

Since the adjoint of a bra/ket is the corresponding ket/bra, and the adjoint of a scalar is its complex conjugate, this simplifies to:

$$
\begin{equation}
\hat{E}^\dagger = \sum_i \dyad{E_i} E_i^*
\end{equation}
$$

Since eigenvalues must be real, the complex conjugate of $E_i$ is just $E_i$.
Plugging this back into the expression for the adjoint, we notice that this is the same as the original operator $\hat{E}$:

$$
\begin{equation}
\hat{E}^\dagger = \sum_i \dyad{E_i} E_i = \hat{E}
\end{equation}
$$

Operators that are equal to their Hermitian adjoints are known as **Hermitian operators**.

The term $\dyad{E_i}$ deserves more attention.
It is known as the **outer product**, and unlike the inner product which is just a scalar, the outer product is an operator.
For a general case $\ket{\psi} \bra{\phi}$, it acts on a state vector $\ket{\chi}$ by first taking the inner product of $\ket{\phi}$ with $\ket{\chi}$ to get a scalar, and then scaling $\ket{\psi}$ by this scalar.
Hence, overall it outputs a state vector.

Because $\dyad{E_i}$ acts by finding the component of a state vector along $\ket{E_i}$ and then scaling $\ket{E_i}$ by this component, it is known as the **projector** onto the eigenvector $\ket{E_i}$.
It is sometimes denoted as $\Lambda_{E_i}$ (e.g. in Sakurai).

## Expectation Value of an Observable

The expectation value of an observable $\hat{A}$ in a state $\ket{\psi}$ is the average value of the observable when measured many times.
It is denoted as $\expval{\hat{A}}$.

In order to compute it, recall that the expectation value for any probability density function $\rho(x)$ is given by:

$$
\begin{equation}
\expval{x} = \int \rho(x) x \dd{x}
\end{equation}
$$

In quantum mechanics, the expectation value of an observable $\hat{A}$ is given by:

$$
\begin{equation}
\expval{\hat{A}} = \expval{\hat{A}}{\psi}
\end{equation}
$$

It is technically a postulate, but we can see why this is the case.
We will do it in a continuous case, but it also applies to the discrete case.

Recall the completeness relation for the eigenvectors of a continuous observable:

$$
\begin{equation}
\int \dyad{x} \dd{x} = \hat{I}
\end{equation}
$$

We can freely insert this relation into any expression.
Let's put it into the right-hand side of the expectation value:

$$
\begin{equation}
\begin{split}
\expval{\hat{x}} &= \mel{\psi}{\hat{x}}{\psi} \\
&= \bra{\psi} \hat{x} \class{blue}{\int \dyad{x} \dd{x}} \ket{\psi} \\
&= \class{blue}{\int} \bra{\psi} \hat{x} \class{blue}{\dyad{x}} \ket{\psi} \class{blue}{\dd{x}} \\
&= \int \mel{\psi}{\hat{x}}{x} \braket{x}{\psi} \dd{x} \\
\end{split}
\end{equation}
$$

Next, we can insert another completeness relation for the left-hand side ($\bra{\psi}$).
To avoid confusion with variables, we will use $x'$ as the variable of integration:

$$
\begin{equation}
\begin{split}
\expval{\hat{x}} &= \int \mel{\psi}{\hat{x}}{x} \braket{x}{\psi} \dd{x} \\
&= \int \mel{\psi \class{yellow}{\int \ket{x'} \bra{x'} \dd{x'}}}{\hat{x}}{x} \braket{x}{\psi} \dd{x} \\
&= \int \class{yellow}{\int} \bra{\psi} \class{yellow}{\ket{x'} \bra{x'}} \hat{x} \ket{x} \braket{x}{\psi} \dd{x} \class{yellow}{\dd{x'}}
\end{split}
\end{equation}
$$

Now, we have a $\hat{x} \ket{x}$ term. Recall that $\ket{x}$ is an eigenvector of the position operator, so this is just a scaled vector $x \ket{x}$:

$$
\begin{equation}
\expval{\hat{x}} = \iint \braket{\psi}{x'} x \braket{x'}{x} \braket{x}{\psi} \dd{x} \dd{x'}
\end{equation}
$$

There is a delta function in the middle, $\braket{x'}{x} = \delta(x' - x)$.
This is only nonzero when $x' = x$, so we can replace $x'$ with $x$ everywhere and integrate over $x$:

$$
\begin{equation}
\expval{\hat{x}} = \int \braket{\psi}{x} x \braket{x}{\psi} \dd{x}
\end{equation}
$$

By conjugate symmetry, $\braket{x}{\psi} = \braket{\psi}{x}^*$. We can then combine both inner products to get the square magnitude (since $z^* z = |z|^2$):

$$
\begin{equation}
\expval{\hat{x}} = \int \braket{\psi}{x} x \braket{x}{\psi} \dd{x} = \int |\braket{x}{\psi}|^2 x \dd{x}
\end{equation}
$$

Recall from the Born rule that $|\braket{x}{\psi}|^2$ is the probability density function for the position operator.
Thus, the expectation value of the position operator does indeed match the definition of the expectation value for a probability density function:

$$
\begin{equation}
\expval{\hat{x}} = \int \underbrace{|\braket{x}{\psi}|^2}_{\small\text{Probability density $\rho(x)$}} x \dd{x}
\end{equation}
$$

## Summary and Next Steps

In this chapter, we have introduced the concept of observables in quantum mechanics.
Observables are quantities that can be measured, and they are represented by Hermitian operators.

Here are the key points to remember:

- Observables are quantities that can be measured in quantum mechanics. They correspond to "real-life" or physical quantities.
- Their eigenvectors form a basis representing the possible outcomes of the observable, and their eigenvalues are the values of the observable.
- After making a measurement, the state vector $\ket{\psi}$ collapses to the eigenvector corresponding to the measured eigenvalue.
- By intuition, observables have the following three properties:

    1. The eigenvalues of an observable are real numbers.
    2. The eigenvectors of an observable form a complete basis.
    3. The eigenvectors of an observable are orthogonal.
- The operators can be written as a sum of their eigenvectors and eigenvalues:

    $$
    \begin{equation}
    \hat{E} = \sum_i E_i \dyad{E_i} \tag{\ref{eq:operator_expansion_discrete}}
    \end{equation}
    $$

    In the continuous case, this becomes an integral:

    $$
    \begin{equation}
    \hat{x} = \int x \dyad{x} \dd{x} \tag{\ref{eq:operator_expansion_continuous}}
    \end{equation}
    $$
- The *operator* $\dyad{E_i}$ is known as the projector onto the eigenvector $\ket{E_i}$. It is sometimes denoted as $\Lambda_{E_i}$.
- The Born rule states that the probability of measuring an observable with eigenvalue $E_i$ is $|\braket{E_i}{\psi}|^2$.
    It followed intuitively from a few key insights about the nature of observables and the conditions for probabilities.
    Namely, they must sum to $1$ in all bases, and they must preserve the norm of the state vector in all bases.
- The adjoint of an operator is the operator that is used for the dual correspondence between vectors and linear functionals.
    The adjoint of $\hat{A}$ is denoted as $\hat{A}^\dagger$, and is defined such that $\braket{\phi}{\hat{A} \psi} = \braket{\hat{A}^\dagger \phi}{\psi}$.
- Operators that are equal to their Hermitian adjoints are known as Hermitian operators.
- Hermitian adjoints have the following properties:

    1. Applying the Hermitian adjoint twice gives the original operator.
    2. The adjoint of a sum of operators is the sum of the adjoints.
    3. The adjoint of a product of operators is the product of the adjoints in reverse order.
    4. The adjoint of a scalar is its complex conjugate.
    5. The adjoint of an operator "flips" its input and output spaces.
- Operators representing observables are Hermitian operators.
- The expectation value of an observable $\hat{A}$ in a state $\ket{\psi}$ is the average value of the observable when measured many times.
    It is denoted as $\expval{\hat{A}}$ and is given by $\expval{\hat{A}} = \expval{\hat{A}}{\psi}$.

In the next chapter, we will take a brief detour to discuss Poisson brackets in classical mechanics, and then link them to commutators in quantum mechanics, from which we will derive the uncertainty principle.
Afterwards, we will discuss how operators can be represented as matrices, and how they can be diagonalized to find their eigenvectors and eigenvalues.

### References

- Quantum Sense, "Maths of Quantum Mechanics", a [Youtube Playlist](https://www.youtube.com/playlist?list=PL8ER5-vAoiHAWm1UcZsiauUGPlJChgNXC).
- J.J. Sakurai, "Modern Quantum Mechanics", sections 1.2-1.4.
- [This post](https://math.stackexchange.com/questions/343910/is-a-bra-the-adjoint-of-a-ket) on Math Stack Exchange.

## Appendix: Review of Eigenvalues and Eigenvectors

An **eigenvector** of a linear transformation is a nonzero vector that changes at most by a scalar factor when that linear transformation is applied to it.
The scalar factor is known as the **eigenvalue** corresponding to that eigenvector.

Let $\vb{A}$ be a linear transformation. If it has an eigenvector $\va{v}$ with eigenvalue $\lambda$, then:

$$
\begin{equation}
\vb{A} \va{v} = \lambda \va{v}
\end{equation}
$$

Rearranging, we get:

$$
\begin{equation}
(\vb{A} - \lambda \vb{I}) \va{v} = \va{0}
\end{equation}
$$

where $\vb{I}$ is the identity matrix.
In order for this equation to hold, the matrix $\vb{A} - \lambda \vb{I}$ must be singular, which means its determinant is zero.
Intuitively, a zero determinant means that the matrix squishes space down to a lower dimension, which is why the vector goes to zero.
Thus:

$$
\begin{equation}
\det(\vb{A} - \lambda \vb{I}) = 0
\end{equation}
$$

This equation is known as the **characteristic equation** of the matrix $\vb{A}$.

There is a shortcut to finding the eigenvalues of a matrix.
If $m$ is half the trace of the matrix (the sum of the diagonal elements) and $p$ is the determinant of the matrix, then the eigenvalues are:

$$
\begin{equation}
\lambda = m \pm \sqrt{m^2 - p}
\end{equation}
$$

This comes from the fact that the trace is the sum of the eigenvalues and the determinant is the product of the eigenvalues (one can see this geometrically by considering the area of the parallelogram formed by the eigenvectors).

## Appendix: Why is the Hermitian Adjoint of a Ket its Corresponding Bra?

This proof is a bit more abstract, and it comes from [this post](https://math.stackexchange.com/questions/343910/is-a-bra-the-adjoint-of-a-ket) on Math Stack Exchange.
It borrows idaes from functional analysis and the dual correspondence between vectors and linear functionals.

**Theorem**: In a finite-dimensional Hilbert space $\mathcal{H}$, the Hermitian adjoint of a ket vector $\ket{\psi}$ is its corresponding bra vector $\bra{\psi}$, which is the linear functional that is dual to the inner product $\braket{\psi}{\cdot}$.

**Proof**. Recall that Hermitian adjoints act on operators, not vectors.
It would thus seem like we need to somehow interpret a vector as a linear transformation.
This comes from our dual correspondence, where a vector can be thought of as a linear functional.
In fact, let's define this formally - for a vector $\ket{\psi}$, we can write its corresponding linear functional as $\Phi[\ket{\psi}] = \bra{\psi}$.
$\Phi[\ket{\psi}]$ is a linear functional that takes a scalar and outputs a vector. The simplest form of $\Phi[\ket{\psi}]$ is just to have it scale the vector by the input scalar:

$$
\begin{equation}
\Phi[\ket{\psi}](c) = c \ket{\psi}
\end{equation}
$$

With this definition, $\Phi$ is known as the **Canonical isomorphism** between a Hilbert space $\mathcal{H}$ and its dual space $\mathcal{H}^*$.

Now that we have an idea of what it means to interpret a vector as a linear transformation, we can see what it means to take the adjoint of said transformation.
Since this transformation is $\mathbb{C} \to \mathcal{H}$, its adjoint is $\mathcal{H} \to \mathbb{C}$ (by the 5th property of the Hermitian adjoint).
We already know that linear functionals are also $\mathcal{H} \to \mathbb{C}$, so it seems like the adjoint of a vector is a bra (linear functional).
But is it exactly the bra $\bra{\psi}$, the one that corresponds to $\ket{\psi}$?
We can see this by taking the product of the adjoint of a vector with another vector (which should output a scalar):

$$
\Psi[\bra{\psi}]^\dagger \ket{\phi} \in \mathbb{C}
$$

Since $\mathbb{C}$ is also an inner product space, it is the same thing as $\ev{1, \Psi[\bra{\psi}]^\dagger \ket{\phi}}_{\mathbb{C}}$, where I added a $\mathbb{C}$ subscript to the inner product to denote that it is the inner product in $\mathbb{C}$.
By the definition of the Hermitian adjoint, this is equal to $\ev{\Psi[\bra{\psi}] (1), \ket{\phi}}_{\mathcal{H}}$ (notice that the inner product changed from $\mathbb{C}$ to $\mathcal{H}$).
But looking at the left-hand side of the inner product, recall that applying the adjoint of a vector to a scalar just scales the vector by the scalar.
So all this is equal to $\ev{\bra{\psi}, \ket{\phi}}_{\mathcal{H}}$, which is *precisely* the inner product of $\bra{\psi}$ and $\ket{\phi}$.
Thus, $\braket{\psi}{\phi} = \ket{\psi}^\dagger \ket{\phi}$, and so $\ket{\psi}^\dagger = \bra{\psi}$. $\blacksquare$

One might rightfully ask how this can be extended to infinite-dimensional Hilbert spaces.
This is easier to see if we restrict ourselves to continuous linear functionals.
Then, the Riesz representation theorem automatically gives us a bra vector for every ket vector.

Outside of continuous linear functionals, the correspondence is not as clear.
I will not delve into this, but the author of this post has a good explanation of this in a comment on the linked post.
It involves concepts like dense subspaces and topologies, so be prepared for some heavy mathematics.
