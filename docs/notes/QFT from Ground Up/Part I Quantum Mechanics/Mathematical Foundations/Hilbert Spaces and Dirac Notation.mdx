---
sidebar_position: 1
---

import { Mafs, Coordinates, Plot, Line, Circle, Theme, useMovablePoint, useStopwatch, vec, Vector, LaTeX, Polygon, Transform } from "mafs";

import { useState, useCallback } from "react";
import { lineLabel } from "@site/src/utilities/lines";
import { color } from "@site/src/utilities/colors"
import TOCInline from '@theme/TOCInline';
import * as MB from "mathbox-react"
import * as THREE from "three"
import { OrbitControls } from "three/examples/jsm/controls/OrbitControls"

# Hilbert Spaces and Dirac Notation

Our first section on quantum mechanics will introduce the fundamental concepts of quantum mechanics.
In this chapter, we will introduce the state vector formalism, which is the most fundamental concept in quantum mechanics.

Through this chapter, we shall gradually build up the mathematical framework of quantum mechanics.
At the same time, we will slowly develop our notation to a more abstract and concise form.
This means, for example, we will eventually strip away the arrow on top of a vector: $\va{v} \to \vb{v}$.
Operators will also slowly lose their hats: $\hat{A} \to A$.

## Table of Contents

<TOCInline toc={toc} />

## How do We Represent States?

Classical physics relies on a set of assumptions about how states of a system are represented.
These include:

- A quantity like $x$ (position) is just a single number.
- Quantities can never have multiple values simultaneously. A particle cannot be at two places at once, nor can it have two different momenta.
- The state of a system is completely determined by specifying the values of all such quantities.
- These quantities are continuous, meaning that there are no gaps between possible values.
    This is intuitively true because particles cannot just jump instantly from one position to another.
- The state of a system can be known with arbitrary precision.

These assumptions make it natural to represent the state of a system using a **continous function**.
For example, the position of a particle can be represented by a function $x(t)$ that gives the position of the particle at each time $t$.

Quantum mechanics, however, challenges these assumptions:

- It introduces the concept of **superposition**, where a particle can exist in multiple states at once.
    For example, a particle can be in a superposition of being at two different positions simultaneously.
- It introduces the concept of **quantization**, where certain quantities can only take on discrete values.
    For example, the energy of an electron in an atom can only take on certain discrete values, or, if you have read the section on the [Stern Gerlach Experiment](../Stern%20Gerlach%20Experiment), the spin of an electron can only be up or down.
- It introduces the concept of **uncertainty**, where the state of a system cannot be known with arbitrary precision.
    This is encapsulated in the various uncertainty principles.

Clearly, we cannot use continuous functions to represent the state of a system in quantum mechanics.

Suppose that we want to write down the state of a particle using its energy (for example, the energy of an electron in an atom).
Assume that, at an energy level of exactly $E_1$, the particle's state is described by some object $\ket{1}$.
We do not currently know what this object is, but we know that it represents the state of the particle at energy level $E_1$.

Of course, there are multiple outcomes for the energy of the particle, each corresponding to a different state $\ket{i}$.
We can write the state of the particle by combining these states with an unknown operation:

$$
\ket{1} \cdot \ket{2} \cdot \ket{3} \cdot \ldots
$$

Each state also has its own probability - some states are more likely than others.
Hence, it's also appropriate to introduce a scaling factor $c_i$ for each state $\ket{i}$:

$$
c_1 \ket{1} \cdot c_2 \ket{2} \cdot c_3 \ket{3} \cdot \ldots
$$

It turns out, the operation is just addition, and introducing scale factors makes the state a **linear combination** of the states $\ket{i}$.
Since we can add and scale the states $\ket{i}$, they form a **vector space**, and $\ket{i}$ is a **vector** in this space.
A vector inside these brackets, $\ket{a}$, is called a **ket**.

This is the fundamental idea behind the **state vector** formalism in quantum mechanics.
The state of a system is represented by a vector in a vector space, and the state can be a linear combination of multiple basis states.
The reason we do not use the usual notation for vectors (like $\vec{\psi}$) will become clear later.

If the representation of *everything* about a system with object is not clear, know that this is a common pattern in physics.
Later on, we will see that we can do the same in classical mechanics, where we can represent everything about a system with the Lagrangian or Hamiltonian.

## The State Vector (and a Need for Cauchy Completeness)

In quantum mechanics, we represent physical quantities like position, momentum, and energy using **operators**.
As shown previously, the state of a system is represented by a state vector $\ket{\psi}$.
One might guess that to find some physical quantity of the system, we can use a matrix to operate on the state vector.
This matrix is called an **operator**.

For example, the position operator $\hat{x}$ acts on the state vector $\ket{\psi}$ to give the position of the particle.
It is denoted as $\hat{x} \ket{\psi}$.

The state vector $\ket{\psi}$ is a vector representing the entire state of the system.
As shown previously, it is formed by combining different states $\ket{i}$ with scaling factors $c_i$:

$$
\begin{equation}
\ket{\psi} = c_1 \ket{E_1} + c_2 \ket{E_2} + c_3 \ket{E_3} + c_4 \ket{E_4}
\end{equation}
$$

This sum is called a **superposition** of states.

But of course, some quantities can take on an infinite number of values.
A particle can theoretically take on any energy level, so we need to sum over an infinite number of states:

$$
\begin{equation}
\ket{\psi} = c_1 \ket{E_1} + c_2 \ket{E_2} + c_3 \ket{E_3} + \ldots = \sum_i c_i \ket{E_i}
\end{equation}
$$

For a continuous quantity, like position, we need to sum over an uncountably infinite number of states.
This means that the sum becomes an integral:

$$
\begin{equation}
\ket{\psi} = \int c(x) \ket{x} \dd{x}
\end{equation}
$$

It turns out that $c(x)$ is the **position wavefunction**, $\psi(x)$.
For another quantity, like momentum, we would have a different wavefunction $\psi(p)$, called the **momentum wavefunction**.
Therefore, for any physical quantity $q$, we can define a wavefunction that represents the state of the system in that quantity's space:

<Boxed>
$$
\begin{equation}
\ket{\psi} = \int \psi(q) \ket{q} \dd{q} \label{eq:continuous_basis_combination}
\end{equation}
$$
</Boxed>

The state vectors that are used to represent the state of the system are called **basis states**.
Recall from linear algebra that the dimensionality of a vector space is the number of basis states required to span the space.
But if there are an infinite number of basis states, the vector space is infinite-dimensional.
This raises a problem. To illustrate, consider a vector space represented by the set of all polynomials.
The basis states are the monomials $1, x, x^2, x^3, \ldots$, and polynomials are linear combinations of these basis states.
But observe this following infinite polynomial:

$$
1 + \frac{1}{2!} x + \frac{1}{3!} x^2 + \frac{1}{4!} x^3 + \ldots
$$

This infinite polynomial turns out to be $e^x$, which is **not a polynomial**.
Hence, an infinite sum of polynomials can give a function that is not a polynomial.

In order for the wavefunction $\psi(q)$ to be a valid representation of the state of the system, we must add one more condition: a limit of a sequence/series of state vectors must converge to a state vector in the space.
This is known as **Cauchy completeness**, and it ensures that the space of state vectors is valid.

Let's summarize the properties of the vector space of state vectors:

- The state of a system is represented by a state vector $\ket{\psi}$.
- The state vector is a linear combination of basis states $\ket{q}$.
- The basis states form an infinite-dimensional vector space.
- The vector space is Cauchy complete, meaning that the limit of a sequence of state vectors converges to a state vector in the space.

This vector space that we have just described is *almost* the definition of a **Hilbert space** $\mathcal{H}$.
We need to add one more property to make it a Hilbert space: the space has a well-defined **inner product**.

### States of Coupled Systems

Suppose we have a particle that can be in one of two states, $\ket{\uparrow}$ and $\ket{\downarrow}$. (Think of the spin of an electron.)
The state of the particle can be represented by a vector in a two-dimensional vector space, where the basis states are $\ket{\uparrow}$ and $\ket{\downarrow}$.

$$
\begin{equation}
\ket{\psi} = c_1 \ket{\uparrow} + c_2 \ket{\downarrow}
\end{equation}
$$

Now, suppose we add another particle to this system, which can also be in one of two states, $\ket{\uparrow}$ and $\ket{\downarrow}$. (Think of another electron.)
In this case, then, there are now four possible states to represent all the different combinations of states of the two particles.
For instance, if both particles are in the $\ket{\uparrow}$ state, the state of the system is $\ket{\uparrow} \otimes \ket{\uparrow}$, where $\otimes$ is something we will define later.
Hence, the basis states of the system are $\ket{\uparrow} \otimes \ket{\uparrow}$, $\ket{\uparrow} \otimes \ket{\downarrow}$, $\ket{\downarrow} \otimes \ket{\uparrow}$, and $\ket{\downarrow} \otimes \ket{\downarrow}$.

The vector *space* is now four-dimensional, which can be thought of as the "product" of the two two-dimensional spaces.
If $\ket{\psi}$ is a state vector in this space, it can be written as a linear combination of the basis states:

$$
\begin{equation}
\ket{\psi} = c_1 \ket{\uparrow} \otimes \ket{\uparrow} + c_2 \ket{\uparrow} \otimes \ket{\downarrow} + c_3 \ket{\downarrow} \otimes \ket{\uparrow} + c_4 \ket{\downarrow} \otimes \ket{\downarrow}
\end{equation}
$$

For brevity, the basis states are often written as $\ket{\uparrow} \ket{\uparrow}$ or simply $\ket{\uparrow \uparrow}$:

$$
\begin{equation}
\ket{\psi} = c_1 \ket{\uparrow \uparrow} + c_2 \ket{\uparrow \downarrow} + c_3 \ket{\downarrow \uparrow} + c_4 \ket{\downarrow \downarrow}
\end{equation}
$$

In a similar fashion to the vectors, the vector *space* of the system is written as $\mathcal{H}_1 \otimes \mathcal{H}_2$, where $\mathcal{H}_1$ and $\mathcal{H}_2$ are the vector spaces of the two particles.

It turns out that this $\otimes$ operation is the **tensor product**.
The tensor product of two vector spaces $\mathcal{H}_1$ and $\mathcal{H}_2$ is a vector space $\mathcal{H}_1 \otimes \mathcal{H}_2$ that contains all possible combinations of states of the two vector spaces.
The basis states of the tensor product space are the tensor products of the basis states of the two vector spaces.
We will explore the tensor product in more detail later.

This framework is known as the **composite system postulate**.
To give an example of its significance, consider two electrons.
Electrons are spin-1/2 fermions, which means that they obey the Pauli exclusion principle (two fermions cannot occupy the same state).
However, at very low temperatures, the two electrons form a Cooper pair, which is a bound state of two electrons.
This bound state is the tensor product of the individual states, and is now a boson with spin-0.
Thus they no longer have to obey the Pauli exclusion principle, and can occupy the same state.
This forms the basis of superconductivity.

## The Inner Product

Another important concept in quantum mechanics is the **inner product**.
It is a generalization of the dot product in Euclidean space to infinite-dimensional spaces.

Recall that the [dot product](/notes/maths/Linear%20Algebra/Vectors%20II/Dot%20Product%20Introduction) of two vectors $\va{a}$ and $\va{b}$ (in an orthonormal basis) is given by:

$$
\begin{equation}
\va{a} \cdot \va{b} = \norm{\va{a}} \norm{\va{b}} \cos \theta = a_x b_x + a_y b_y + a_z b_z
\end{equation}
$$

:::note

Outside orthonormal bases, the dot product is given by $\va{a} \cdot \va{b} = \va{a}^T \vb{G} \va{b} = a^\mu g_{\mu\nu} b^\nu$, where $\vb{G} = g_{\mu\nu} \vb{e}^\mu \otimes \vb{e}^\nu$ is the metric tensor.

:::

Defining a dot product helps us define angles between vectors, as well as lengths and projections.
However, it seems that the product does not have an equally simple geometric interpretation in abstract vector spaces like the space of state vectors.

Suppose $\ket{\psi}$ and $\ket{\phi}$ are two state vectors in the space of state vectors.
The inner product is denoted as $\braket{\psi}{\phi}$.
Just like the dot product, the inner product is a way to multiply two vectors to get a scalar.
An inner product requires the following properties, all of which should be familiar from the dot product:

1. **Linearity in the second argument**: $\braket{\psi}{\alpha \phi + \beta \chi} = \alpha \braket{\psi}{\phi} + \beta \braket{\psi}{\chi}$.
2. **Conjugate symmetry**: $\braket{\psi}{\phi} = \braket{\phi}{\psi}^*$.
    There is a reason for the conjugation, and we can show this with a simple example.

    Consider an inner product of $\ket{\psi}$ (with a magnitude of $1$) with itself: $\braket{\psi}{\psi} = 1$.
    Next, scale the vectors in the inner product by $i$ to get $\braket{i \psi}{i \psi}$. By linearity, this is $i^2 \braket{\psi}{\psi} = -1$.
    But the inner product defines the magnitude, and with this scaling, the magnitude is $\sqrt{\braket{i \psi}{i \psi}} = i$.

    Hence, to ensure that the magnitude is real, we need to conjugate one of the vectors when flipping the order: $\braket{\psi}{\phi} = \braket{\phi}{\psi}^*$.
    The following shows that the magnitude of $\braket{i \psi}{i \psi}$ is real:

    $$
    \begin{equation}
    \braket{i \psi}{i \psi} = i\braket{i \psi}{\psi} = i\braket{\psi}{i \psi}^* = ii^*\braket{\psi}{\psi} = ii^* = 1
    \end{equation}
    $$
3. **Positive definiteness**: If $\ket{\psi} \neq \ket{0}$, then $\braket{\psi}{\psi} > 0$.
    This ensures that the inner product is a valid measure of the magnitude of a vector.
    After all, the magnitude of a vector should not be zero if the vector itself is not the zero vector.

### Corollary: Antilinearity in the First Argument

For the linearity of the first argument, we can show that it is **antilinear** from the linearity of the second argument and conjugate symmetry:

$$
\begin{equation}
\begin{split}
\braket{\alpha \psi + \beta \chi}{\phi} &= \braket{\phi}{\alpha \psi + \beta \chi}^* \\
&= \qty(\alpha \braket{\phi}{\psi} + \beta \braket{\phi}{\chi})^* \\
&= \alpha^* \braket{\phi}{\psi}^* + \beta^* \braket{\phi}{\chi}^* \\
&= \alpha^* \braket{\psi}{\phi} + \beta^* \braket{\chi}{\phi}
\end{split}
\end{equation}
$$

### Corollary: Magnitude and Orthogonality

- The magnitude of a vector $\ket{\psi}$ is given by $\sqrt{\braket{\psi}{\psi}}$, just like the magnitude of a vector in Euclidean space.
- Two vectors $\ket{\psi}$ and $\ket{\phi}$ are orthogonal if $\braket{\psi}{\phi} = 0$.

### Applying to Discrete Basis States

Let's apply the inner product to quantum mechanics, where the basis states are discrete.
In an orthnormal basis, the inner product of two basis states $\ket{i}$ and $\ket{j}$ is given by:

$$
\begin{equation}
\braket{i}{j} = \delta_{ij}
\end{equation}
$$

This is the **Kronecker delta**, which is $1$ if $i = j$ and $0$ otherwise.

Recall that the state vector $\ket{\psi}$ is a linear combination of basis states $\ket{i}$:

$$
\begin{equation}
\ket{\psi} = \sum_i c_i \ket{i} \label{eq:discrete_basis_combination}
\end{equation}
$$

Suppose we want to find the coefficient $c_j$. This is similar to finding the component of a vector along a direction in Euclidean space.
We can find $c_j$ by taking the inner product of $\ket{\psi}$ with $\ket{j}$:

$$
\begin{equation}
\braket{j}{\psi} = \sum_i c_i \braket{j}{i} = \sum_i c_i \delta_{ij} = c_j
\end{equation}
$$

Therefore:

$$
\begin{equation}
c_j = \braket{j}{\psi} \label{eq:orthonormal_coefficient}
\end{equation}
$$

This is the **projection** of $\ket{\psi}$ onto $\ket{j}$.

## Dirac Delta Function

Previously, we applied the inner product to discrete basis states.
But what if the basis states are continuous, like the position basis $\ket{x}$?

Recall that the state vector $\ket{\psi}$ can be written as an integral of basis states $\ket{x}$:

$$
\begin{equation}
\ket{\psi} = \int \psi(x) \ket{x} \dd{x} \tag{\ref{eq:continuous_basis_combination}}
\end{equation}
$$

Suppose we want to find the coefficient $\psi(x)$ when the basis state is $\ket{x'}$ (where $x'$ is a specific value of $x$).
We can find $\psi(x')$ by taking the inner product of $\ket{\psi}$ with $\ket{x'}$, just like we did with discrete basis states:

$$
\begin{equation}
\braket{x'}{\psi} = \int \psi(x) \braket{x'}{x} \dd{x}
\end{equation}
$$

The inner product $\braket{x'}{x}$ is a function that is $0$ everywhere except at $x = x'$, where it is infinite.
(This is because the basis states $\ket{x}$ are orthogonal to each other, except when $x = x'$.)
This function is the **Dirac delta function** $\delta(x - x')$.

### What is the Dirac Delta Function?

The Dirac Delta can be thought of as a continuous analog of the Kronecker delta.
A common interpretation of the Dirac delta function is that it is a function that is zero everywhere except at a single point, where it is infinite:

$$
\begin{equation}
\delta(x) = \begin{cases}
\infty & \text{if } x = 0 \\
0 & \text{otherwise}
\end{cases}
\end{equation}
$$

Additionally, it satisfies the following property:

$$
\begin{equation}
\int_{-\infty}^{\infty} \delta(x) \dd{x} = 1
\end{equation}
$$

Intuitively, the Dirac delta function is like a infinitely big spike at a single point.
We can shift the spike to any point $x'$ by writing $\delta(x - x')$.
The integral property is then:

$$
\begin{equation}
\int_{-\infty}^{\infty} \delta(x - x') \dd{x} = 1
\end{equation}
$$

We can also multiply the integrand by a function $f(x)$. The only time when the integrand is non-zero is when $x = x'$, so the integral is just $f(x')$:

$$
\begin{equation}
\int_{-\infty}^{\infty} f(x) \delta(x - x') \dd{x} = f(x')
\end{equation}
$$

However, the big-spike interpretation is not the complete picture.
The Dirac delta function can be constructed as the limit of a function.
Consider the normalized Gaussian function:

$$
\begin{equation}
\rho_{\varepsilon}(x) = \frac{1}{\sqrt{2\pi \varepsilon^2}} e^{-x^2 / 2\varepsilon^2}
\end{equation}
$$

This function is a bell curve centered at $x = 0$ with a width of $\varepsilon$.
As $\varepsilon \to 0$, the bell curve becomes narrower and taller, and the area under the curve remains $1$, as required for a probability density function.
Thus, the limit of $\rho_{\varepsilon}(x)$ as $\varepsilon \to 0$ is the Dirac delta function.

The graph of this distribution is shown below:

{(() => {
    const [epsilon, setEpsilon] = useState(1);
    const distribution = (x) => 1 / Math.sqrt(2 * Math.PI * epsilon ** 2) * Math.exp(-(x ** 2) / (2 * epsilon ** 2));
    return (
        <div className="card">
            <div className="card__header">
                <h3>Normalized Gaussian Distribution</h3>
            </div>
            <div className="card__body">
                <Mafs>
                    <Coordinates.Cartesian />
                    <Plot.OfX y={distribution} />
                </Mafs>
            </div>
            <div className="card__footer">
                <ControlPanel>
                    <SliderInput latex label="\varepsilon" value={epsilon} onChange={setEpsilon} min={0.01} max={2} step={0.01} />
                </ControlPanel>
            </div>
        </div>
    )
})()}

Next, consider another function:

$$
\begin{equation}
\rho_{\varepsilon}(x) = \frac{1}{\pi x} \sin(\frac{x}{\varepsilon})
\end{equation}
$$

This function is a sine wave that oscillates faster as $\varepsilon$ decreases.
A graph is also shown below:

{(() => {
    const [epsilon, setEpsilon] = useState(1);
    const distribution = (x) => 1 / (Math.PI * x) * Math.sin(x / epsilon);
    return (
        <div className="card">
            <div className="card__header">
                <h3>Sine Distribution</h3>
            </div>
            <div className="card__body">
                <Mafs>
                    <Coordinates.Cartesian />
                    <Plot.OfX y={distribution} />
                </Mafs>
            </div>
            <div className="card__footer">
                <ControlPanel>
                    <SliderInput latex label="\varepsilon" value={epsilon} onChange={setEpsilon} min={0.01} max={2} step={0.01} />
                </ControlPanel>
            </div>
        </div>
    )
})()}

As $\varepsilon \to 0$, the sine wave oscillates faster and faster.
It does have a big spike at $x = 0$, but it also oscillates at other points.
However, it turns out that the limit of this function as $\varepsilon \to 0$ is also the Dirac delta function because it satisfies its integral property.
This function is used extensively in the Fourier transform.

Instead, a better definition of the Dirac delta function is the following:

<Boxed>
The Dirac delta function $\delta(x)$ is a distribution that satisfies the following property:

$$
\begin{equation}
\int_{-\infty}^{\infty} f(x) \delta(x - x') \dd{x} = f(x')
\end{equation}
$$
</Boxed>

### Corollary: Inner Product of Wavefunctions

Consider the inner product of two wavefunctions $\psi(x)$ and $\phi(x)$.
Each wavefunction can be written as an integral of basis states $\ket{x}$:

$$
\begin{align}
\ket{\psi} &= \int \psi(x) \ket{x} \dd{x} \\
\ket{\phi} &= \int \phi(y) \ket{y} \dd{y}
\end{align}
$$

(We use $y$ in the second wavefunction to avoid confusion with $x$.)
The inner product of these wavefunctions is then:

$$
\begin{equation}
\begin{split}
\braket{\psi}{\phi} &= \qty(\int \psi(x) \ket{x} \dd{x}) \qty(\int \phi(y) \ket{y} \dd{y}) \\
&= \iint \psi(x)^* \phi(y) \braket{x}{y} \dd{x} \dd{y} \\
&= \iint \psi(x)^* \phi(y) \delta(x - y) \dd{x} \dd{y}
\end{split}
\end{equation}
$$

Since this is only nonzero when $x = y$, we can collapse the $y$ integral and just replace $y$ with $x$:

$$
\begin{equation}
\braket{\psi}{\phi} = \int \psi(x)^* \phi(x) \dd{x}
\end{equation}
$$

This is similar to the inner product in the discrete case:

$$
\begin{equation}
\braket{\psi}{\phi} = \sum_i \psi_i^* \phi_i
\end{equation}
$$

### Back to the Inner Product

Going back to trying to find $\psi(x')$ from the inner product $\braket{x'}{\psi}$, we can write it as:

$$
\begin{equation}
\braket{x'}{\psi} = \int \psi(x) \braket{x'}{x} \dd{x} = \int \psi(x) \delta(x - x') \dd{x}
\end{equation}
$$

This is just $\psi(x')$:

$$
\begin{equation}
\braket{x'}{\psi} = \int \psi(x) \delta(x - x') \dd{x} = \psi(x')
\end{equation}
$$

## Dual Space and Bra Vectors

Consider a linear map $A$ that acts on a vector $\ket{\psi}$ to give a scalar $a$:

$$
\begin{equation}
A \ket{\psi} = a
\end{equation}
$$

This type of linear map is called a **linear functional**.
For example, consider an Euclidean example: define $L_x$ as a linear functional that acts on a vector $\va{v}$ to give the $x$-component of $\va{v}$.
Then, $L_x \va{v} = v_x$.

Linear functionals can be represented by a $1 \times n$ matrix, where $n$ is the dimension of the vector space.
In other words, they are row vectors.
These objects have many names - linear functionals, covectors, dual vectors, and row vectors.
For a more detailed explanation and some visual intuition, see [this Eigenchris video](https://www.youtube.com/watch?v=LNoQ_Q5JQMY).
It details how linear functionals can be visualized by contour lines.

The set of all linear functionals on a vector space forms a vector space itself, called the **dual space** (denoted as $\mathcal{V}^*$).
Linear functionals appear in quantum mechanics because we need to convert from the vector (state vector) to a scalar (e.g. probability, expectation value, etc.).
In quantum mechanics, the dual space is denoted as $\bra{\psi}$ (called a **bra** vector), and they exist in the dual Hilbert space $\mathcal{H}^*$.
A linear functional $A$ acting on a state vector $\ket{\psi}$ is denoted as $\bra{A} {} \ket{\psi}$.

Notice that inner products, just like linear functionals, map a vector to a scalar.
This means that inner products and linear functionals are fundamentally the same.
For any linear functional, you can find a corresponding vector that represents it in an inner product.
You can watch [this 3b1b video](https://www.youtube.com/watch?v=LyGKycYT2v0) for a more visual explanation.
This property is known as the **Riesz representation theorem**:

<Boxed>
For any linear functional $A$ in a Hilbert space $\mathcal{H}$, there exists a unique vector $\ket{A}$ in $\mathcal{H}$ such that:

$$
\begin{equation}
A(\ket{\psi}) = \braket{A}{\psi}
\end{equation}
$$
</Boxed>

To show this in our notation, we literally write the linear functional as an inner product:

$$
\begin{equation}
\bra{A} {} \ket{\psi} = \braket{A}{\psi}
\end{equation}
$$

This set of notation is called **bra-ket notation** or **Dirac notation**.
It is very helpful because it allows us to interchange between vectors and linear functionals easily without having to worry about the details.

### Resolution of the Identity

The **resolution of the identity** is a property of the basis states in a Hilbert space.
We will prove this in order to show how Dirac notation simplifies calculations.

Consider a set of orthonormal basis states $\ket{A_i}$.
A quantum state $\ket{\psi}$ can be written as a linear combination of these basis states:

$$
\begin{equation}
\ket{\psi} = \sum_i c_i \ket{A_i}
\end{equation}
$$

In an orthonormal basis, $c_i$ can be found by taking the inner product of $\ket{\psi}$ with $\ket{A_i}$ (from Equation $\eqref{eq:orthonormal_coefficient}$):

$$
\begin{equation}
\ket{\psi} = \sum_i \class{blue}{\braket{A_i}{\psi}} \ket{A_i} = \sum_i \ket{A_i} \class{blue}{\braket{A_i}{\psi}}
\end{equation}
$$

Dirac notation allows us to split the braket into two parts:

$$
\begin{equation}
\ket{\psi} = \sum_i \dyad{A_i} {} \ket{\psi}
\end{equation}
$$

Since $\ket{\psi}$ is constant regardless of the summation index, we can write it outside the sum:

$$
\begin{equation}
\ket{\psi} = \qty(\sum_i \dyad{A_i}) \ket{\psi}
\end{equation}
$$

This is an equation that holds for any state $\ket{\psi}$.
This must mean that the quantity in the parentheses is the identity operator $\hat{I}$:

<Boxed>
The **resolution of the identity** states that:

$$
\begin{equation}
\hat{I} = \sum_i \dyad{A_i} \label{eq:resolution_identity}
\end{equation}
$$
</Boxed>

This is a very important property of the basis states in a Hilbert space.
It is also known as the **completeness relation**.
In the continuous case, the sum becomes an integral:

$$
\begin{equation}
\hat{I} = \int \dyad{q} \dd{q} \label{eq:resolution_identity_continuous}
\end{equation}
$$

## Summary and Next Steps

In this note, we have introduced the concept of the state vector in quantum mechanics.
The state of a system is represented by a state vector $\ket{\psi}$, which is a linear combination of basis states $\ket{i}$.

Here are the key points to remember:

- The state of a system is represented by a state vector $\ket{\psi}$ in a Hilbert space $\mathcal{H}$.
- The state of a coupled system is represented by a state vector in the tensor product space $\mathcal{H}_1 \otimes \mathcal{H}_2$.
- A Hilbert space is an infinite-dimensional vector space that is Cauchy complete.
- The state vector is a linear combination of basis states $\ket{i}$.
    In a discrete basis, the state vector is a sum over basis states:

    $$
    \begin{equation}
    \ket{\psi} = \sum_i c_i \ket{i} \tag{\ref{eq:discrete_basis_combination}}
    \end{equation}
    $$

    In a continuous basis, the state vector is an integral over basis states:

    $$
    \begin{equation}
    \ket{\psi} = \int \psi(q) \ket{q} \dd{q} \tag{\ref{eq:continuous_basis_combination}}
    \end{equation}
    $$

- The inner product of two state vectors $\ket{\psi}$ and $\ket{\phi}$ is an operation that gives a scalar.
    It is denoted as $\braket{\psi}{\phi}$ and satisfies these properties:

    1. Linearity in the second argument: $\braket{\psi}{\alpha \phi + \beta \chi} = \alpha \braket{\psi}{\phi} + \beta \braket{\psi}{\chi}$.
    2. Conjugate symmetry: $\braket{\psi}{\phi} = \braket{\phi}{\psi}^*$.
    3. Positive definiteness: $\braket{\psi}{\psi} > 0$ if $\ket{\psi} \neq \ket{0}$.

- The Dirac delta function $\delta(x)$ is a distribution that satisfies the property:

    $$
    \begin{equation}
    \int_{-\infty}^{\infty} f(x) \delta(x - x') \dd{x} = f(x')
    \end{equation}
    $$

    It can be interpreted as a big spike at a single point, but that is not the complete picture.

- The value of $\psi(x')$ can be found by taking the inner product of $\ket{\psi}$ with $\ket{x'}$:

    $$
    \begin{equation}
    \braket{x'}{\psi} = \int \psi(x) \delta(x - x') \dd{x} = \psi(x')
    \end{equation}
    $$

- Linear functionals in a Hilbert space are represented by bra vectors $\bra{\psi}$.
    They exist in the dual Hilbert space $\mathcal{H}^*$.
    The inner product of a bra vector and a state vector is denoted as $\braket{\psi}{\phi}$.
    This forms the basis of Dirac notation.

- The resolution of the identity/completeness relation states that the identity operator $\hat{I}$ can be written as:

    $$
    \begin{equation}
    \hat{I} = \sum_i \ket{A_i} \bra{A_i} \tag{\ref{eq:resolution_identity}}
    \end{equation}
    $$

    In the continuous case, the sum becomes an integral:

    $$
    \begin{equation}
    \hat{I} = \int \ket{q} \bra{q} \dd{q} \tag{\ref{eq:resolution_identity_continuous}}
    \end{equation}
    $$

This was a lot of information and it is completely normal for the reader to feel overwhelmed.
While I only used one page to explain these concepts, it took physicists decades to develop them, not mentioning the centuries of mathematical development that preceded them.
Furthermore, quantum mechanics itself is a very complex theory that opposes human intuition.

In the next note, we will move on to how operators act on state vectors and how they relate to observable quantities.

### References

- Quantum Sense, "Maths of Quantum Mechanics", a [Youtube Playlist](https://www.youtube.com/playlist?list=PL8ER5-vAoiHAWm1UcZsiauUGPlJChgNXC).
- J.J. Sakurai, "Modern Quantum Mechanics", section 1.4.
